<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>CEI Literature Vault — LLM Ethics Dashboard</title>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/echarts@5.5.1/dist/echarts.min.js"></script>
<style>
/* ── CSS Reset & Custom Properties ─────────────────────────────────── */
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
:root {
  --bg-primary: #09090b;
  --bg-card: #18181b;
  --bg-card-hover: #27272a;
  --bg-surface: #111113;
  --border: #3f3f46;
  --border-subtle: #27272a;
  --text-primary: #fafafa;
  --text-secondary: #a1a1aa;
  --text-muted: #71717a;
  --accent: #6366f1;
  --accent-light: #818cf8;
  --accent-dim: rgba(99,102,241,0.15);
  --success: #22c55e;
  --warning: #f59e0b;
  --danger: #ef4444;
  --cat-1: #6366f1; --cat-2: #22d3ee; --cat-3: #f59e0b;
  --cat-4: #ec4899; --cat-5: #22c55e; --cat-6: #a855f7;
  --cat-7: #f97316; --cat-8: #14b8a6; --cat-9: #e879f9;
  --cat-10: #38bdf8;
  --radius: 12px;
  --radius-sm: 8px;
  --font-sans: 'Inter', -apple-system, sans-serif;
  --font-mono: 'JetBrains Mono', 'SF Mono', monospace;
}
html { scroll-behavior: smooth; }
body {
  font-family: var(--font-sans);
  background: var(--bg-primary);
  color: var(--text-primary);
  line-height: 1.6;
  min-height: 100vh;
}
/* ── Header ────────────────────────────────────────────────────────── */
.header {
  background: var(--bg-surface);
  border-bottom: 1px solid var(--border-subtle);
  padding: 20px 24px 0;
  position: sticky;
  top: 0;
  z-index: 100;
  backdrop-filter: blur(12px);
}
.header-content {
  max-width: 1400px;
  margin: 0 auto;
}
.header h1 {
  font-size: 1.25rem;
  font-weight: 600;
  color: var(--text-primary);
  margin-bottom: 4px;
}
.header .subtitle {
  font-size: 0.8rem;
  color: var(--text-muted);
  margin-bottom: 16px;
}
/* ── Tabs ──────────────────────────────────────────────────────────── */
.tabs {
  display: flex;
  gap: 0;
  overflow-x: auto;
  scrollbar-width: none;
}
.tabs::-webkit-scrollbar { display: none; }
.tab {
  padding: 10px 20px;
  font-size: 0.85rem;
  font-weight: 500;
  color: var(--text-muted);
  cursor: pointer;
  border-bottom: 2px solid transparent;
  transition: all 0.2s;
  white-space: nowrap;
  user-select: none;
}
.tab:hover { color: var(--text-secondary); }
.tab.active {
  color: var(--accent-light);
  border-bottom-color: var(--accent);
}
/* ── Main ──────────────────────────────────────────────────────────── */
.main {
  max-width: 1400px;
  margin: 0 auto;
  padding: 24px;
}
.section {
  display: none;
  animation: fadeIn 0.3s ease;
}
.section.active { display: block; }
@keyframes fadeIn {
  from { opacity: 0; transform: translateY(8px); }
  to { opacity: 1; transform: translateY(0); }
}
/* ── Grid ──────────────────────────────────────────────────────────── */
.grid { display: grid; gap: 20px; }
.grid-2 { grid-template-columns: repeat(auto-fit, minmax(400px, 1fr)); }
.grid-5 { grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); }
@media (max-width: 768px) {
  .grid-2, .grid-5 { grid-template-columns: 1fr; }
}
/* ── Cards ─────────────────────────────────────────────────────────── */
.card {
  background: var(--bg-card);
  border: 1px solid var(--border-subtle);
  border-radius: var(--radius);
  padding: 20px;
  transition: border-color 0.2s;
}
.card:hover { border-color: var(--border); }
.card-title {
  font-size: 0.8rem;
  font-weight: 500;
  color: var(--text-muted);
  text-transform: uppercase;
  letter-spacing: 0.05em;
  margin-bottom: 12px;
}
/* ── KPI Cards ─────────────────────────────────────────────────────── */
.kpi {
  background: var(--bg-card);
  border: 1px solid var(--border-subtle);
  border-radius: var(--radius);
  padding: 20px 24px;
  position: relative;
  overflow: hidden;
}
.kpi::before {
  content: '';
  position: absolute;
  left: 0;
  top: 0;
  bottom: 0;
  width: 3px;
  background: var(--accent);
}
.kpi-value {
  font-family: var(--font-mono);
  font-size: 2rem;
  font-weight: 600;
  color: var(--text-primary);
  line-height: 1.2;
}
.kpi-label {
  font-size: 0.78rem;
  color: var(--text-muted);
  margin-top: 4px;
}
.kpi:nth-child(2)::before { background: var(--cat-2); }
.kpi:nth-child(3)::before { background: var(--success); }
.kpi:nth-child(4)::before { background: var(--cat-3); }
.kpi:nth-child(5)::before { background: var(--cat-6); }
.kpi:nth-child(6)::before { background: var(--cat-2); }
/* ── Chart containers ──────────────────────────────────────────────── */
.chart-box {
  width: 100%;
  height: 400px;
}
.chart-box-sm { height: 320px; }
.chart-box-lg { height: 500px; }
.chart-box-xl { height: 600px; }
/* ── Table ─────────────────────────────────────────────────────────── */
.table-wrap {
  overflow-x: auto;
  border-radius: var(--radius);
  border: 1px solid var(--border-subtle);
}
table {
  width: 100%;
  border-collapse: collapse;
  font-size: 0.82rem;
}
th {
  background: var(--bg-card);
  color: var(--text-secondary);
  font-weight: 500;
  text-align: left;
  padding: 10px 14px;
  border-bottom: 1px solid var(--border);
  position: sticky;
  top: 0;
  cursor: pointer;
  user-select: none;
  white-space: nowrap;
}
th:hover { color: var(--text-primary); }
td {
  padding: 8px 14px;
  border-bottom: 1px solid var(--border-subtle);
  color: var(--text-secondary);
  max-width: 320px;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}
tr:hover td { background: var(--bg-card-hover); }
tr:nth-child(even) td { background: rgba(24,24,27,0.5); }
tr:nth-child(even):hover td { background: var(--bg-card-hover); }
/* ── Filters ───────────────────────────────────────────────────────── */
.filters {
  display: flex;
  gap: 12px;
  flex-wrap: wrap;
  align-items: center;
  margin-bottom: 16px;
}
.filter-input {
  background: var(--bg-card);
  border: 1px solid var(--border);
  border-radius: var(--radius-sm);
  color: var(--text-primary);
  padding: 8px 14px;
  font-size: 0.82rem;
  font-family: var(--font-sans);
  outline: none;
  transition: border-color 0.2s;
  min-width: 200px;
}
.filter-input:focus { border-color: var(--accent); }
select.filter-input { cursor: pointer; }
.filter-count {
  font-size: 0.78rem;
  color: var(--text-muted);
  margin-left: auto;
}
/* ── Pagination ────────────────────────────────────────────────────── */
.pagination {
  display: flex;
  gap: 4px;
  justify-content: center;
  margin-top: 16px;
  align-items: center;
}
.page-btn {
  background: var(--bg-card);
  border: 1px solid var(--border-subtle);
  border-radius: 6px;
  color: var(--text-secondary);
  padding: 6px 12px;
  font-size: 0.78rem;
  cursor: pointer;
  transition: all 0.2s;
}
.page-btn:hover { border-color: var(--border); color: var(--text-primary); }
.page-btn.active {
  background: var(--accent);
  border-color: var(--accent);
  color: white;
}
.page-btn:disabled { opacity: 0.3; cursor: default; }
/* ── Normative cards ───────────────────────────────────────────────── */
.norm-card {
  background: var(--bg-card);
  border: 1px solid var(--border-subtle);
  border-radius: var(--radius);
  padding: 16px 20px;
  margin-bottom: 12px;
}
.norm-card h4 {
  font-size: 0.85rem;
  color: var(--accent-light);
  margin-bottom: 8px;
  word-break: break-word;
}
.norm-card .meta {
  font-size: 0.78rem;
  color: var(--text-muted);
  line-height: 1.6;
}
.norm-card .meta span {
  color: var(--text-secondary);
}
/* ── Badge ─────────────────────────────────────────────────────────── */
.badge {
  display: inline-block;
  font-size: 0.7rem;
  padding: 2px 8px;
  border-radius: 99px;
  font-weight: 500;
}
.badge-public { background: rgba(34,197,94,0.15); color: var(--success); }
.badge-code { background: rgba(99,102,241,0.15); color: var(--accent-light); }
.badge-partial { background: rgba(245,158,11,0.15); color: var(--warning); }
/* ── Footer ────────────────────────────────────────────────────────── */
.footer {
  text-align: center;
  padding: 32px 24px;
  font-size: 0.75rem;
  color: var(--text-muted);
  border-top: 1px solid var(--border-subtle);
  margin-top: 48px;
}
</style>
</head>
<body>

<!-- ── Header & Tabs ────────────────────────────────────────────────── -->
<div class="header">
  <div class="header-content">
    <h1>CEI Literature Vault — LLM Ethics & Values Alignment</h1>
    <div class="subtitle">924 papers screened &bull; 528 benchmark candidates &bull; Generated 2026-02-12</div>
    <div class="tabs" id="tabs">
      <div class="tab active" data-tab="overview">Overview</div>
      <div class="tab" data-tab="taxonomy">Taxonomy</div>
      <div class="tab" data-tab="models">Models & Methods</div>
      <div class="tab" data-tab="cultural">Cultural Coverage</div>
      <div class="tab" data-tab="resources">Resources</div>
      <div class="tab" data-tab="normative">Normative</div>
      <div class="tab" data-tab="llm">LLM Assessment</div>
    </div>
  </div>
</div>

<!-- ── Main Content ─────────────────────────────────────────────────── -->
<div class="main">

<!-- ═══ Section 1: Overview ═══════════════════════════════════════════ -->
<div class="section active" id="sec-overview">
  <div class="grid grid-5" style="margin-bottom:24px" id="kpi-row"></div>
  <div class="grid grid-2">
    <div class="card"><div class="card-title">Papers Over Time</div><div class="chart-box" id="chart-timeline"></div></div>
    <div class="card"><div class="card-title">Category Distribution</div><div class="chart-box" id="chart-categories"></div></div>
    <div class="card"><div class="card-title">Data Availability</div><div class="chart-box chart-box-sm" id="chart-availability"></div></div>
    <div class="card"><div class="card-title">Top Prolific Authors</div><div class="chart-box chart-box-sm" id="chart-authors"></div></div>
  </div>
</div>

<!-- ═══ Section 2: Taxonomy ══════════════════════════════════════════ -->
<div class="section" id="sec-taxonomy">
  <div class="grid grid-2" style="margin-bottom:20px">
    <div class="card"><div class="card-title">Category x Year Heatmap</div><div class="chart-box chart-box-lg" id="chart-cat-year"></div></div>
    <div class="card"><div class="card-title">Co-occurrence Matrix</div><div class="chart-box chart-box-lg" id="chart-cooccurrence"></div></div>
  </div>
</div>

<!-- ═══ Section 3: Models & Methods ══════════════════════════════════ -->
<div class="section" id="sec-models">
  <div class="grid grid-2" style="margin-bottom:20px">
    <div class="card"><div class="card-title">Model Family Prevalence</div><div class="chart-box" id="chart-model-bar"></div></div>
    <div class="card"><div class="card-title">Methodology Prevalence</div><div class="chart-box" id="chart-method-bar"></div></div>
  </div>
  <div class="card"><div class="card-title">Model x Category Heatmap</div><div class="chart-box chart-box-lg" id="chart-model-cat"></div></div>
</div>

<!-- ═══ Section 4: Cultural Coverage ═════════════════════════════════ -->
<div class="section" id="sec-cultural">
  <div class="grid grid-2" style="margin-bottom:20px">
    <div class="card"><div class="card-title">Pluralistic Relevance</div><div class="chart-box chart-box-sm" id="chart-pluralistic"></div></div>
    <div class="card"><div class="card-title">Ethical Tradition Coverage</div><div class="chart-box" id="chart-traditions"></div></div>
  </div>
  <div class="grid grid-2">
    <div class="card"><div class="card-title">Language Coverage</div><div class="chart-box" id="chart-languages"></div></div>
    <div class="card"><div class="card-title">Language x Category Heatmap</div><div class="chart-box chart-box-lg" id="chart-lang-cat"></div></div>
  </div>
</div>

<!-- ═══ Section 5: Resources ═════════════════════════════════════════ -->
<div class="section" id="sec-resources">
  <div class="filters">
    <input type="text" class="filter-input" id="res-search" placeholder="Search benchmarks...">
    <select class="filter-input" id="res-cat" style="min-width:180px">
      <option value="">All Categories</option>
    </select>
    <select class="filter-input" id="res-year" style="min-width:120px">
      <option value="">All Years</option>
    </select>
    <label style="font-size:0.82rem;color:var(--text-secondary);display:flex;align-items:center;gap:6px;">
      <input type="checkbox" id="res-public"> Public only
    </label>
    <div class="filter-count" id="res-count"></div>
  </div>
  <div class="table-wrap"><table id="res-table">
    <thead><tr>
      <th data-sort="year">Year</th>
      <th data-sort="benchmark">Benchmark</th>
      <th data-sort="what">What It Measures</th>
      <th data-sort="category">Category</th>
      <th data-sort="availability">Availability</th>
    </tr></thead>
    <tbody id="res-tbody"></tbody>
  </table></div>
  <div class="pagination" id="res-pagination"></div>
</div>

<!-- ═══ Section 6: Normative ═════════════════════════════════════════ -->
<div class="section" id="sec-normative">
  <div class="grid grid-2" style="margin-bottom:20px">
    <div class="card"><div class="card-title">Normative Classification</div><div class="chart-box chart-box-sm" id="chart-norm-donut"></div></div>
    <div class="card"><div class="card-title">Framework Distribution</div><div class="chart-box" id="chart-norm-fw"></div></div>
  </div>
  <div class="card">
    <div class="card-title">Papers with Full Normative Operationalization</div>
    <div id="norm-papers"></div>
  </div>
</div>

<!-- ═══ Section 7: LLM Assessment ═════════════════════════════════════ -->
<div class="section" id="sec-llm">
  <div class="grid grid-5" style="margin-bottom:24px">
    <div class="kpi"><div class="kpi-value" style="color:#22d3ee" id="llm-kpi-models">0</div><div class="kpi-label">Papers with LLM Models</div></div>
    <div class="kpi"><div class="kpi-value" style="color:#22c55e" id="llm-kpi-urls">0</div><div class="kpi-label">Papers with Data URLs</div></div>
    <div class="kpi"><div class="kpi-value" style="color:#f59e0b" id="llm-kpi-pct">0%</div><div class="kpi-label">LLM Coverage Rate</div></div>
  </div>
  <div class="grid grid-2" style="margin-bottom:20px">
    <div class="card"><div class="card-title">LLM Model Frequency (Top 20)</div><div class="chart-box chart-box-lg" id="chart-llm-models"></div></div>
    <div class="card"><div class="card-title">Assessment Coverage by Year</div><div class="chart-box chart-box-lg" id="chart-llm-year"></div></div>
  </div>
  <div class="grid grid-2">
    <div class="card"><div class="card-title">Data Repository Distribution</div><div class="chart-box" id="chart-llm-repos"></div></div>
  </div>
</div>

</div><!-- /main -->

<div class="footer">
  CEI Literature Vault &bull; Generated 2026-02-12 &bull; 924 papers &bull; Script: generate_vault_dashboard.py
</div>

<!-- ═══ DATA & SCRIPTS ══════════════════════════════════════════════ -->
<script>
const DATA = {"kpi":{"total":924,"candidates":528,"public":287,"code":19,"pluralisticPct":76,"normative":45,"normativeYes":7,"normativeBorderline":38,"authors":846,"llmModels":566,"llmModelsPct":67,"dataUrls":452,"totalPapers":846},"timeline":{"2016":{"all":6,"cand":1},"2018":{"all":13,"cand":7},"2019":{"all":12,"cand":7},"2020":{"all":21,"cand":11},"2021":{"all":24,"cand":10},"2022":{"all":45,"cand":26},"2023":{"all":117,"cand":51},"2024":{"all":284,"cand":160},"2025":{"all":377,"cand":237},"2026":{"all":23,"cand":18}},"categories":[{"id":1,"name":"1. Moral Reasoning & Ethical Judgment","short":"Moral Reasoning & Ethical Judgment","count":268,"byYear":{"2018":4,"2019":1,"2020":6,"2021":3,"2022":14,"2023":27,"2024":67,"2025":137,"2026":9}},{"id":2,"name":"2. Value Alignment & Value Pluralism","short":"Value Alignment & Value Pluralism","count":150,"byYear":{"2021":3,"2022":5,"2023":15,"2024":47,"2025":74,"2026":6}},{"id":3,"name":"3. Cultural & Cross-Cultural Ethics","short":"Cultural & Cross-Cultural Ethics","count":193,"byYear":{"2018":1,"2019":2,"2020":4,"2021":1,"2022":4,"2023":22,"2024":61,"2025":90,"2026":8}},{"id":4,"name":"4. Fairness, Bias & Social Norms","short":"Fairness, Bias & Social Norms","count":233,"byYear":{"2016":1,"2018":4,"2019":4,"2020":9,"2021":6,"2022":14,"2023":23,"2024":66,"2025":99,"2026":7}},{"id":5,"name":"5. Toxicity, Hate Speech & Harmful Content","short":"Toxicity, Hate Speech & Harmful Content","count":94,"byYear":{"2018":2,"2019":5,"2020":3,"2021":2,"2022":12,"2023":7,"2024":29,"2025":33,"2026":1}},{"id":6,"name":"6. Safety & Red Teaming","short":"Safety & Red Teaming","count":181,"byYear":{"2019":1,"2021":1,"2022":9,"2023":17,"2024":65,"2025":81,"2026":7}},{"id":7,"name":"7. Domain-Specific Ethics","short":"Domain-Specific Ethics","count":117,"byYear":{"2018":2,"2019":1,"2020":2,"2022":3,"2023":12,"2024":35,"2025":59,"2026":3}},{"id":8,"name":"8. Normative Ethics Benchmarks","short":"Normative Ethics Benchmarks","count":65,"byYear":{"2020":3,"2021":2,"2022":3,"2023":7,"2024":21,"2025":29}},{"id":9,"name":"9. Moral Psychology Applied to AI","short":"Moral Psychology Applied to AI","count":62,"byYear":{"2018":1,"2019":1,"2020":3,"2022":3,"2023":6,"2024":18,"2025":26,"2026":4}},{"id":10,"name":"10. Helpfulness, Honesty & RLHF","short":"Helpfulness, Honesty & RLHF","count":299,"byYear":{"2018":3,"2020":1,"2021":3,"2022":19,"2023":38,"2024":86,"2025":138,"2026":11}}],"cooccurrence":[[0,49,84,114,24,59,75,64,58,144],[49,0,72,68,11,38,30,10,12,115],[84,72,0,83,29,60,40,16,21,121],[114,68,83,0,49,70,50,33,31,121],[24,11,29,49,0,59,12,1,5,41],[59,38,60,70,59,0,38,11,14,117],[75,30,40,50,12,38,0,22,20,75],[64,10,16,33,1,11,22,0,14,42],[58,12,21,31,5,14,20,14,0,26],[144,115,121,121,41,117,75,42,26,0]],"models":[{"name":"Llama","count":72,"byCategory":[47,23,25,34,11,19,19,12,15,44],"byYear":{"2022":2,"2023":4,"2024":29,"2025":37}},{"name":"GPT-4","count":68,"byCategory":[44,18,20,23,10,21,20,11,14,34],"byYear":{"2022":1,"2023":11,"2024":29,"2025":27}},{"name":"GPT-3.5","count":42,"byCategory":[28,11,17,28,6,8,13,10,12,15],"byYear":{"2022":1,"2023":16,"2024":16,"2025":9}},{"name":"Gemini","count":40,"byCategory":[32,5,12,14,5,12,17,7,6,22],"byYear":{"2024":9,"2025":31}},{"name":"Claude","count":36,"byCategory":[24,12,7,14,2,8,11,7,5,22],"byYear":{"2023":1,"2024":10,"2025":25}},{"name":"Mistral/Mixtral","count":26,"byCategory":[17,9,10,11,3,7,4,4,6,15],"byYear":{"2024":10,"2025":16}},{"name":"BERT family","count":25,"byCategory":[13,4,10,13,5,0,5,4,3,8],"byYear":{"2019":2,"2020":3,"2021":4,"2022":2,"2023":3,"2024":6,"2025":5}},{"name":"DeepSeek","count":20,"byCategory":[15,5,7,7,2,6,5,4,3,9],"byYear":{"2023":1,"2025":19}},{"name":"GPT-3","count":10,"byCategory":[6,2,1,7,3,3,2,2,3,6],"byYear":{"2021":1,"2022":3,"2023":5,"2025":1}},{"name":"GPT-2","count":9,"byCategory":[3,3,3,7,3,1,2,0,2,3],"byYear":{"2020":3,"2021":2,"2022":2,"2024":1,"2025":1}},{"name":"Qwen","count":8,"byCategory":[5,4,6,1,1,3,3,2,1,5],"byYear":{"2023":1,"2024":2,"2025":5}},{"name":"Gemma","count":8,"byCategory":[7,3,4,4,1,3,2,1,1,6],"byYear":{"2024":1,"2025":7}},{"name":"Delphi","count":7,"byCategory":[4,3,2,4,0,1,3,2,1,5],"byYear":{"2022":2,"2023":3,"2024":1,"2025":1}},{"name":"Vicuna","count":5,"byCategory":[1,1,1,2,2,3,0,0,0,1],"byYear":{"2024":2,"2025":3}},{"name":"T5/Flan","count":4,"byCategory":[4,0,2,0,0,0,0,0,0,2],"byYear":{"2022":1,"2023":2,"2024":1}},{"name":"PaLM","count":4,"byCategory":[4,0,1,2,0,0,1,2,2,0],"byYear":{"2023":2,"2024":1,"2025":1}},{"name":"ChatGLM","count":4,"byCategory":[2,1,3,2,0,1,1,1,0,2],"byYear":{"2023":1,"2024":1,"2025":2}},{"name":"Alpaca","count":3,"byCategory":[1,1,1,2,1,2,1,0,0,1],"byYear":{"2023":2,"2024":1}},{"name":"Falcon","count":3,"byCategory":[2,0,1,1,1,1,0,0,0,2],"byYear":{"2024":2,"2025":1}},{"name":"OPT","count":2,"byCategory":[1,2,2,1,1,0,0,0,0,2],"byYear":{"2022":1,"2025":1}}],"methods":[{"name":"Scenario/vignette judgment","count":179,"byCategory":[133,44,54,66,12,43,45,42,31,102]},{"name":"Human preference/comparison","count":150,"byCategory":[73,56,45,67,26,53,38,13,22,113]},{"name":"Classification task","count":85,"byCategory":[45,15,31,37,30,29,13,10,13,37]},{"name":"Benchmark suite (multi-task)","count":67,"byCategory":[28,14,22,28,24,40,18,4,6,31]},{"name":"Adversarial/red-teaming","count":55,"byCategory":[15,10,19,16,21,53,9,2,2,31]},{"name":"Multi-turn dialogue","count":46,"byCategory":[17,14,12,17,10,20,11,4,2,42]},{"name":"Survey/questionnaire","count":41,"byCategory":[21,30,23,21,4,11,9,3,13,28]},{"name":"Multiple-choice QA","count":39,"byCategory":[9,14,19,13,3,18,12,2,1,23]},{"name":"Template/probing","count":38,"byCategory":[21,13,14,23,9,11,9,5,5,20]},{"name":"Game/simulation","count":37,"byCategory":[22,6,5,10,2,11,7,8,3,24]},{"name":"Free-text generation","count":37,"byCategory":[20,15,11,19,7,10,4,4,2,20]},{"name":"Crowdsourced annotation","count":24,"byCategory":[17,4,9,13,2,5,7,1,7,10]},{"name":"Likert/scale rating","count":15,"byCategory":[6,6,7,9,1,5,4,1,5,6]},{"name":"Embedding analysis","count":6,"byCategory":[2,3,1,4,1,1,0,1,1,4]}],"pluralistic":{"Yes":205,"Partial":198,"No":125},"traditions":[{"name":"Consequentialist","count":20,"level":"Adequate"},{"name":"Islamic","count":13,"level":"Adequate"},{"name":"Care Ethics","count":3,"level":"Severe"},{"name":"Confucian","count":2,"level":"Severe"},{"name":"Indigenous","count":2,"level":"Severe"},{"name":"Virtue Ethics","count":2,"level":"Severe"},{"name":"Deontological","count":2,"level":"Severe"},{"name":"Buddhist","count":1,"level":"Severe"},{"name":"Hindu","count":1,"level":"Severe"},{"name":"Ubuntu","count":0,"level":"Critical"}],"languages":[{"name":"Chinese","count":40,"byCategory":[19,11,40,10,5,20,11,5,3,24]},{"name":"English","count":34,"byCategory":[12,6,26,17,9,8,3,2,4,16]},{"name":"Japanese","count":10,"byCategory":[7,1,10,5,1,2,3,1,1,6]},{"name":"Arabic","count":9,"byCategory":[2,4,9,2,1,3,3,0,1,5]},{"name":"Spanish","count":8,"byCategory":[5,0,7,4,2,1,0,1,3,0]},{"name":"German","count":6,"byCategory":[0,3,2,4,4,3,2,0,0,3]},{"name":"Hindi","count":5,"byCategory":[3,0,5,1,2,1,0,1,1,1]},{"name":"Russian","count":5,"byCategory":[5,0,4,3,0,0,1,2,2,0]},{"name":"French","count":4,"byCategory":[3,0,3,2,1,1,0,0,1,2]},{"name":"Indonesian","count":3,"byCategory":[0,0,3,0,2,2,0,0,0,0]},{"name":"Korean","count":3,"byCategory":[0,1,3,2,0,0,0,0,0,2]},{"name":"Italian","count":3,"byCategory":[2,1,2,0,2,1,1,0,1,2]},{"name":"Portuguese","count":2,"byCategory":[2,0,1,0,1,0,0,0,2,1]},{"name":"Bengali","count":2,"byCategory":[0,0,1,1,1,0,0,0,0,0]},{"name":"Swahili","count":2,"byCategory":[2,0,2,1,0,0,0,1,1,0]},{"name":"Urdu","count":2,"byCategory":[0,0,2,1,2,0,0,0,0,0]},{"name":"Dutch","count":1,"byCategory":[0,1,0,1,0,1,1,0,0,1]},{"name":"Vietnamese","count":1,"byCategory":[1,1,1,0,0,0,0,0,0,1]}],"availability":{"Public":287,"Code available":19,"Not stated":222,"Partially public":0,"Not public":0},"normative":{"yes":7,"borderline":38,"excluded":221,"frameworks":[{"name":"Consequentialism/Utilitarianism","count":27},{"name":"Deontology/Kantian","count":26},{"name":"Virtue Ethics","count":8},{"name":"Principlism","count":8},{"name":"Contractualism/Rawlsian","count":6},{"name":"Commonsense Morality","count":3},{"name":"Moral Foundations Theory","count":3},{"name":"Kohlberg's Moral Development","count":2},{"name":"Distributive Justice","count":1},{"name":"Confucian Ethics","count":1},{"name":"Social Choice Theory","count":1}],"papers":[{"filename":"2024-Agarwal-Ethical Reasoning and Moral Value Alignment Multilingual.md","frameworks":"Deontology, virtue ethics, consequentialism (three branches of normative ethics ","llms":"GPT-4, ChatGPT, Llama2-Chat-70B","artifact":"Ethical dilemmas and policies from three normative ethics branches, tested acros"},{"filename":"2024-Moore-Intuitions of Compromise - Utilitarianism vs. Contractualism.md","frameworks":"Utilitarianism (Utilitarian Sum), Contractualism (Nash Product/Nash Social Welfa","llms":"GPT-4 and additional LLMs (referenced in supplementary mater","artifact":"Systematically generated value aggregation scenarios (Focused set: 162 disagreem"},{"filename":"2024-retrieved-Cross-Linguistic Moral Preferences in LLMs - Evidence from.md","frameworks":"Distributive justice (Rawlsian/egalitarian frameworks), with domain persona inte","llms":"LLMs tested cross-linguistically (specifics not available in","artifact":"1,201,200 observations across ten professional domains testing distributive just"},{"filename":"2025-Hong-PrinciplismQA Towards Assessing Medical Ethics from Knowledge to.md","frameworks":"Principlism (Beauchamp & Childress): Autonomy, Non-Maleficence, Beneficence, Jus","llms":"Multiple LLMs including frontier closed-source models and me","artifact":"PrinciplismQA - 3,648 questions (MCQ from textbooks + open-ended from case study"},{"filename":"2025-Huang-BehaviorBench Multi-Tier Benchmark for Agent Behavior Editing.md","frameworks":"Deontology, virtue ethics, justice, commonsense morality (from ETHICS dataset); ","llms":"9 open-weight LLMs (LLaMA-2-7B, LLaMA-3-8B, Mistral-7B, etc.","artifact":"BEHAVIORBENCH: 1,001 moral scenarios across 10 datasets including ETHICS (justic"},{"filename":"2025-Takeshita-JETHICS - Japanese Ethics Understanding Evaluation Dataset.md","frameworks":"Utilitarianism, Deontology (role-based obligations + prima facie duty), Virtue E","llms":"GPT-4o, plus multiple non-proprietary Japanese LLMs","artifact":"JETHICS: 78K Japanese moral examples across 5 categories (Utilitarianism, Deonto"},{"filename":"2025-Unknown-Moral Alignment for LLM Agents Intrinsic Rewards.md","frameworks":"Deontological Ethics (Kant, duty-based norms like conditional cooperation), Util","llms":"Gemma2-2b-it fine-tuned with RL using intrinsic moral reward","artifact":"Iterated Prisoner's Dilemma (IPD) environment with moral reward functions encodi"}]},"resources":[{"filename":"2026-Dang-RedBench","year":2026,"benchmark":"RedBench","what":"LLM safety robustness against adversarial prompts across 22 risk categories and ","category":"Safety & Red Teaming","availability":"Public","url":""},{"filename":"2026-Goel-Building Interpretable Models for Moral Decision-Making","year":2026,"benchmark":"Interpretable Moral Machine Transformer","what":"How neural networks encode and process moral decision-making on trolley-style di","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2026-Guey-BiasLab Multilingual Dual-Framing Framework","year":2026,"benchmark":"BiasLab","what":"Output-level (extrinsic) bias in LLMs across demographic, cultural, political, a","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2026-He-GVS-Scale and GVS-Bench GenAI Value Safety Scale","year":2026,"benchmark":"GVS-Scale / GVS-Bench","what":"GenAI value safety across lifecycle-oriented taxonomy of value safety risks; eva","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2026-Jin-MedES Chinese Medical Ethics Scenarios","year":2026,"benchmark":"MedES","what":"Medical ethics alignment of LLMs in Chinese healthcare contexts including clinic","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2026-Karimi-Malekabadi-Theory Trace Card TTC Documentation for Socio-Cognitive Benchmarks","year":2026,"benchmark":"Theory Trace Card (TTC)","what":"Proposes a documentation standard for socio-cognitive evaluations of LLMs includ","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2026-Lee-BiasJailbreak","year":2026,"benchmark":"BiasJailbreak","what":"Ethical biases in LLMs that can be exploited for jailbreaking; differential safe","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2026-Shen-PsychEthicsBench","year":2026,"benchmark":"PsychEthicsBench","what":"Ethical knowledge and behavioral responses of LLMs in mental health contexts bas","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Adilazuarda-From Surveys to Narratives Rethinking Cultural Value Adaptation in","year":2025,"benchmark":"Unnamed (WVS-based cultural value adaptation evaluation)","what":"Cultural value adaptation in LLMs: whether WVS-based training captures cultural ","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Alghamdi-AraTrust - An Evaluation of Trustworthiness for LLMs in Arabic","year":2025,"benchmark":"AraTrust","what":"Trustworthiness of LLMs in Arabic across 9 dimensions: truthfulness, ethics, saf","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2025-Alwajih-Palm","year":2025,"benchmark":"PALM","what":"Cultural sensitivity and inclusivity of LLMs across all 22 Arab countries in bot","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Andriushchenko-AgentHarm","year":2025,"benchmark":"AgentHarm","what":"Harmfulness of LLM agents: compliance with malicious requests across 11 harm cat","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2025-Ashkinaze-Deep Value Benchmark","year":2025,"benchmark":"Deep Value Benchmark (DVB)","what":"Whether LLMs learn fundamental human values (e.g., non-maleficence, justice) or ","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-authors-CCD-Bench Probing Cultural Conflict in LLM Decision-Making","year":2025,"benchmark":"CCD-Bench","what":"LLM decision-making under cross-cultural value conflict; how LLMs adjudicate whe","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Backmann-When Ethics and Payoffs Diverge","year":2025,"benchmark":"MORALSIM","what":"LLM moral behavior in social dilemmas where moral imperatives conflict with rewa","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Backmann-When Ethics and Payoffs Diverge LLM Agents in Morally Charged Social","year":2025,"benchmark":"MORALSIM","what":"LLM moral behavior when ethical norms conflict with payoff-maximizing strategies","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Bae-CharMoral","year":2025,"benchmark":"CharMoral","what":"Moral evolution and dynamic morality of characters in long-form narratives: acti","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Banerjee-SafeInfer","year":2025,"benchmark":"HarmEval","what":"LLM safety: potential misuse scenarios aligned with policies of leading AI tech ","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2025-Becker-The Moralization Corpus","year":2025,"benchmark":"Moralization Corpus","what":"Detection and extraction of moralizations (arguments invoking moral values) in a","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Bouchard-How to Assess Your LLM Use Case for Bias and","year":2025,"benchmark":"LangFair","what":"Bias and fairness in LLM outputs: toxicity, stereotypes, and counterfactual fair","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2025-Bouchekif-QIAS 2025 - Shared Task on Islamic Inheritance Reasoning and","year":2025,"benchmark":"QIAS 2025","what":"LLM reasoning in Islamic jurisprudence: inheritance share calculation (Subtask 1","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2025-Bulla-Large Language Models meet moral values","year":2025,"benchmark":"Unnamed (MFT-based LLM moral evaluation)","what":"LLM ability to detect and classify moral values based on Moral Foundations Theor","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Castricato-PERSONA Pluralistic Alignment Testbed","year":2025,"benchmark":"PERSONA / PERSONA Bench","what":"Pluralistic alignment of LLMs with diverse user values across demographic and id","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2025-Chen-MoVa Towards Generalizable Classification of Human Morals and Values","year":2025,"benchmark":"MoVa","what":"Classification of human morals and values across four theoretically-grounded fra","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Chen-SPICA Scenarios for Pluralistic In-Context Alignment","year":2025,"benchmark":"SPICA","what":"Pluralistic in-context alignment: whether LLMs can steer toward group-level valu","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2025-Cheng-ELEPHANT Social Sycophancy","year":2025,"benchmark":"ELEPHANT","what":"Social sycophancy in LLMs: excessive preservation of user's face (desired self-i","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Chiu-CulturalBench","year":2025,"benchmark":"CulturalBench","what":"LLM cultural knowledge across 45 global regions and 17 topics including food pre","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Chiu-MoReBench","year":2025,"benchmark":"MoReBench / MoReBench-Theory","what":"Procedural and pluralistic moral reasoning in LLMs: identifying moral considerat","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Chiu-Will AI Tell Lies to Save Sick Children","year":2025,"benchmark":"LITMUSVALUES / AIRISKDILEMMAS","what":"AI value prioritization and its connection to risky behaviors (e.g., power seeki","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Chua-RabakBench","year":2025,"benchmark":"RabakBench","what":"Multilingual safety of LLMs in Singapore's linguistic context: Singlish, Chinese","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2025-Elkins-Syntactic Framing Fragility SFF Robustness Evaluation of LLM Ethical","year":2025,"benchmark":"SFF (Syntactic Framing Fragility)","what":"Robustness of LLM ethical judgments to syntactic variations (negation and condit","category":"Moral Reasoning & Ethical Judgment","availability":"Code available","url":""},{"filename":"2025-Elle-Reward Model Perspectives on Demographic Alignment Elle et al.","year":2025,"benchmark":"RMP (Reward Model Perspectives)","what":"Sociodemographic biases in reward models; alignment of RM opinions with differen","category":"Value Alignment & Value Pluralism","availability":"Code available","url":""},{"filename":"2025-Fan-FAIRMT-BENCH","year":2025,"benchmark":"FairMT-Bench","what":"Fairness in LLMs during multi-turn dialogues; bias accumulation, stereotype reco","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2025-Gai-MoralBench A Multi-Faceted Benchmark Gai et al.","year":2025,"benchmark":"MoralBench","what":"Moral reasoning capabilities of LLMs based on Moral Foundations Theory (care, fa","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Galatolo-Beyond Ethical Alignment AMAs","year":2025,"benchmark":"AMAeval (Artificial Moral Assistant Evaluation)","what":"LLM capabilities as Artificial Moral Assistants: deductive and abductive moral r","category":"Moral Reasoning & Ethical Judgment","availability":"Code available","url":""},{"filename":"2025-Ghosh-AEGIS 2.0","year":2025,"benchmark":"AEGIS 2.0","what":"AI content safety across 12 hazard categories with 9 fine-grained subcategories ","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2025-Group-AILuminate","year":2025,"benchmark":"AILuminate v1.0","what":"AI system resistance to prompts eliciting dangerous/illegal/undesirable behavior","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2025-Guo-CARE - Multilingual Human Preference Learning for Cultural Awareness","year":2025,"benchmark":"CARE","what":"Cultural awareness of LLMs through multilingual human preference alignment","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Helwe-Navigating the Political Compass Multilingual 50 Countries","year":2025,"benchmark":"Political Compass Test for LLMs (PCT + 8 Values)","what":"Political and ideological biases in LLMs across 50 countries using the Political","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Hu-LLMs on Trial Evaluating Judicial Fairness for Large Language Models","year":2025,"benchmark":"JudiFair","what":"Judicial fairness of LLMs across 65 labels and 161 values; inconsistency, bias, ","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2025-Huang-BehaviorBench Multi-Tier Benchmark for Agent Behavior Editing","year":2025,"benchmark":"BEHAVIORBENCH","what":"Ethical behavior of LLM-based agents; moral alignment across scenarios of increa","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Huang-DeceptionBench A Comprehensive Benchmark for AI Deception Behaviors","year":2025,"benchmark":"DeceptionBench","what":"Deceptive tendencies in LLMs across societal domains (Economy, Healthcare, Educa","category":"Safety & Red Teaming","availability":"Public","url":""},{"filename":"2025-Huang-On the Trustworthiness of Generative Foundation","year":2025,"benchmark":"TrustGen","what":"Trustworthiness of generative foundation models across multiple dimensions: trut","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2025-Ji-PKU-SafeRLHF","year":2025,"benchmark":"PKU-SafeRLHF","what":"Safety alignment in LLMs - helpfulness vs harmlessness tradeoff across 19 harm c","category":"Safety & Red Teaming","availability":"Public","url":""},{"filename":"2025-Jiang-EMNLP Educator-role Moral and Normative LLMs Profiling","year":2025,"benchmark":"EMNLP (Educator-role Moral and Normative LLMs Profiling)","what":"Moral development stages, personality profiling, and ethical risk of LLMs in edu","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Jiang-Investigating Machine Moral Judgement Through the Delphi Experiment","year":2025,"benchmark":"Delphi / DelphiHYBRID","what":"Machine moral judgment - ability to predict human moral judgments about everyday","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Jiao-LLM Ethics Benchmark","year":2025,"benchmark":"LLM Ethics Benchmark","what":"Moral reasoning capabilities across foundational moral principles, reasoning rob","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Jiao-LLM Ethics Benchmark Three-Dimensional Assessment","year":2025,"benchmark":"LLM Ethics Benchmark","what":"Moral reasoning capabilities of LLMs across three dimensions: foundational moral","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Jotautaite-SpeciesismBench Evaluating LLM Recognition of Speciesist Statements","year":2025,"benchmark":"SpeciesismBench","what":"Speciesist bias in LLMs - recognition and moral evaluation of speciesist stateme","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Jotautaitė-Speciesism in AI","year":2025,"benchmark":"SpeciesismBench","what":"Speciesist bias and moral evaluation of non-human animals in LLMs across three p","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Jourdan-FairTranslate","year":2025,"benchmark":"FairTranslate","what":"Non-binary gender bias in machine translation systems from English to French - i","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Kasu-EthicsMH","year":2025,"benchmark":"EthicsMH (Ethical Reasoning in Mental Health)","what":"Ethical reasoning in mental health contexts - confidentiality, autonomy, benefic","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Khan-Randomness Not Representation The Unreliability of Evaluating","year":2025,"benchmark":"Unnamed (Cultural Alignment Robustness Evaluation)","what":"Reliability of survey-based cultural alignment evaluations of LLMs - tests stabi","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Laukkonen-Contemplative Artificial Intelligence","year":2025,"benchmark":"Unnamed (Contemplative AI evaluation using AILuminate Benchm","what":"Ethical alignment improvement through contemplative wisdom principles - evaluate","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Leteno-Histoires Morales","year":2025,"benchmark":"Histoires Morales","what":"Moral alignment of LLMs in French language; whether models align with moral norm","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Li-LiveSecBench - A Dynamic Event-Driven Safety Benchmark for Chinese","year":2025,"benchmark":"LiveSecBench","what":"Chinese LLM safety across five dimensions: Public Safety, Fairness & Bias, Priva","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2025-Li-MASK","year":2025,"benchmark":"MASK (Measuring AI System Knowledge of Honesty)","what":"LLM honesty vs accuracy - whether models lie under pressure, disentangling truth","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2025-Li-SafetyAnalyst","year":2025,"benchmark":"SafetyAnalyst (duplicate entry)","what":"AI behavior safety through interpretable harm-benefit analysis with steerable sa","category":"Safety & Red Teaming","availability":"Code available","url":""},{"filename":"2025-Li-SafetyAnalyst Interpretable Transparent and Steerable Safety","year":2025,"benchmark":"SafetyAnalyst","what":"AI behavior safety through interpretable harm-benefit analysis; whether AI actio","category":"Safety & Red Teaming","availability":"Public","url":""},{"filename":"2025-Li-T2ISafety","year":2025,"benchmark":"T2ISafety","what":"Safety of text-to-image models across toxicity, fairness/racial bias, and privac","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2025-Lin-MORALISE","year":2025,"benchmark":"MORALISE","what":"Visual moral alignment in Vision-Language Models; ability to detect moral violat","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Liu-Can LLMs Grasp Implicit Cultural Values","year":2025,"benchmark":"CQ-Bench (duplicate entry)","what":"LLMs' metacognitive cultural intelligence and ability to infer implicit cultural","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Liu-CQ-Bench Cultural Intelligence Benchmark for LLMs","year":2025,"benchmark":"CQ-Bench (Cultural Intelligence Benchmark)","what":"LLMs' ability to infer implicit cultural values from natural conversational cont","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Liu-PersuSafety LLM Can be a Dangerous Persuader","year":2025,"benchmark":"PersuSafety","what":"Persuasion safety of LLMs; whether models reject unethical persuasion tasks and ","category":"Toxicity, Hate Speech & Harmful Content","availability":"Code available","url":""},{"filename":"2025-Ma-EthicsSuite Detecting Behavioral Inconsistency in LLM Ethics","year":2025,"benchmark":"EthicsSuite","what":"Behavioral inconsistency in LLM ethics-related suggestions; detects unethical bi","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Machlovi-GuardEval","year":2025,"benchmark":"GuardEval","what":"LLM content moderation safety, fairness, and robustness across 106 fine-grained ","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Mitchell-SHADES","year":2025,"benchmark":"SHADES","what":"Culturally-specific stereotypes learned by LLMs across 20 regions and 16 languag","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2025-Mohammadi-Exploring Cultural Variations in Moral Judgments with Large Language","year":2025,"benchmark":"Cultural Moral Judgment Evaluation (WVS/PEW-based)","what":"Whether LLMs mirror cultural variations in moral attitudes from World Values Sur","category":"Moral Reasoning & Ethical Judgment","availability":"Code available","url":""},{"filename":"2025-Myung-PapersPlease","year":2025,"benchmark":"PapersPlease","what":"LLM decision-making in moral dilemmas involving prioritization of human needs (E","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Nair-Do Language Models Think Consistently","year":2025,"benchmark":"Value Preference Consistency Evaluation","what":"Consistency of LLM value preferences between short-form psychometric tests and l","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Narnaware-SB-Bench","year":2025,"benchmark":"SB-Bench (Stereotype Bias Benchmark) / BBQ-V","what":"Stereotype biases in Large Multimodal Models (LMMs) across 9 social categories u","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2025-Nawale-FairI Tales INDIC-BIAS - Evaluation of Fairness in Indian Contexts","year":2025,"benchmark":"INDIC-BIAS","what":"Fairness of LLMs across 85 Indian identity groups encompassing diverse castes, r","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Pandey-SocialHarmBench","year":2025,"benchmark":"SocialHarmBench","what":"LLM vulnerability to harmful compliance in sociopolitical domains including poli","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Pourbahman-ELAB","year":2025,"benchmark":"ELAB (Extensive LLM Alignment Benchmark)","what":"Alignment of Persian LLMs with safety, fairness, and social norms in Persian lin","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Ravindran-Moral Anchor System A Predictive Framework for AI Value Alignment and","year":2025,"benchmark":"Moral Anchor System (MAS)","what":"Value drift in AI agents — detects, predicts, and mitigates deviation from human","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Raza-HumaniBench","year":2025,"benchmark":"HumaniBench","what":"Alignment of Large Multimodal Models with human-centered values: fairness, ethic","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2025-Ren-MASK Benchmark Disentangling Honesty From Accuracy in AI Systems","year":2025,"benchmark":"MASK","what":"Honesty in LLMs — specifically whether models lie when pressured, disentangled f","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Sachdeva-Normative Evaluation of LLMs with Everyday Moral Dilemmas","year":2025,"benchmark":"AITA Moral Dilemmas Evaluation","what":"LLM moral judgment on everyday ethical dilemmas (blame assignment, moral reasoni","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Saffari-Can I introduce my boyfriend to my grandmother","year":2025,"benchmark":"ISN (Iranian Social Norms)","what":"LLM understanding of Iranian social norms and cultural etiquette","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Samway-Are Language Models Consequentialist or Deontological","year":2025,"benchmark":"MORALLENS","what":"Whether LLM moral reasoning is consequentialist or deontological across trolley-","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Santhosh-IndiCASA - Dataset and Bias Evaluation Framework for LLMs in Indian","year":2025,"benchmark":"IndiCASA","what":"Stereotypical bias in LLMs across Indian demographic axes (caste, gender, religi","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Schacht-Mapping Moral Reasoning Circuits","year":2025,"benchmark":"Moral Reasoning Circuits Dataset","what":"Mechanistic localization of moral reasoning neurons across six Moral Foundations","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Shen-ValueActionLens Mind the Value-Action Gap","year":2025,"benchmark":"ValueActionLens","what":"Value-action gap: discrepancy between LLMs' stated values and their actual value","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Shen-ValueCompass A Framework for Measuring Contextual Value Alignment","year":2025,"benchmark":"ValueCompass","what":"Contextual value alignment between humans and LLMs across countries and applicat","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Shetty-VITAL","year":2025,"benchmark":"VITAL","what":"Pluralistic value alignment in healthcare contexts","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Shetty-VITAL Pluralistic Alignment in Healthcare","year":2025,"benchmark":"VITAL","what":"Pluralistic value alignment in healthcare contexts (cultural, religious, persona","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Shin-RoleConflictBench A Benchmark of Role Conflict Scenarios for","year":2025,"benchmark":"RoleConflictBench","what":"LLM contextual sensitivity in social dilemmas involving role conflicts (competin","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Simhi-ManagerBench Evaluating Safety-Pragmatism Trade-off in Managerial","year":2025,"benchmark":"ManagerBench","what":"Safety-pragmatism trade-off in LLM autonomous agent decision-making (choosing be","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Snoswell-Beyond Verdicts LLM Moral Competence","year":2025,"benchmark":"LLM Moral Competence Taxonomy (survey-based)","what":"Surveys and categorizes 69 LLM ethical evaluation papers (2020-2025), proposing ","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Sorensen-Operationalizing Pluralistic Values","year":2025,"benchmark":"Pluralistic Values Alignment Study","what":"Trade-offs between safety, inclusivity, and model behavior when incorporating pl","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Sun-CASE-Bench","year":2025,"benchmark":"CASE-Bench","what":"Context-aware safety evaluation of LLMs","category":"Safety & Red Teaming","availability":"Public","url":""},{"filename":"2025-Sun-CASE-Bench Context-Aware SafEty Benchmark","year":2025,"benchmark":"CASE-Bench","what":"Context-aware safety of LLMs -- whether models appropriately adjust safety respo","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Takeshita-JETHICS","year":2025,"benchmark":"JETHICS","what":"Japanese ethics understanding evaluation","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Takeshita-JETHICS - Japanese Ethics Understanding Evaluation Dataset","year":2025,"benchmark":"JETHICS","what":"Japanese ethics understanding across normative ethical theories (deontology, uti","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Tennant-Moral Alignment for LLM Agents","year":2025,"benchmark":"Moral Alignment IPD Framework","what":"Moral alignment of LLM agents using deontological and utilitarian reward functio","category":"Moral Reasoning & Ethical Judgment","availability":"Code available","url":""},{"filename":"2025-Tennant-Moral Alignment for LLM Agents Intrinsic Rewards","year":2025,"benchmark":"Moral Alignment via Intrinsic Rewards (IPD-based)","what":"Moral alignment of LLM agents using deontological and utilitarian reward functio","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Trager-MFTCXplain","year":2025,"benchmark":"MFTCXplain","what":"Moral reasoning capabilities of LLMs through hate speech multi-hop explanations ","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Trager-MFTCXplain A Multilingual Benchmark Dataset for Evaluating the Moral","year":2025,"benchmark":"MFTCXplain","what":"Moral reasoning capabilities of LLMs through multi-hop hate speech explanation u","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Various-AISafetyLab","year":2025,"benchmark":"AISafetyLab","what":"AI safety through integrated attack, defense, and evaluation methodologies","category":"Safety & Red Teaming","availability":"Public","url":""},{"filename":"2025-Wang-Measuring Desired Group Discrimination in LLMs","year":2025,"benchmark":"DiffAware Benchmark Suite","what":"Desired group discrimination / difference awareness in LLMs -- when treating gro","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Wang-Multimodal Understanding of Human Values in Videos","year":2025,"benchmark":"TikTok Values Dataset","what":"Human values expressed in TikTok videos using Schwartz Theory of Personal Values","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Wei-MedEthicsQA","year":2025,"benchmark":"MedEthicsQA","what":"Medical ethics reasoning in LLMs","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Wei-MedEthicsQA A Comprehensive Question Answering Benchmark for Medical","year":2025,"benchmark":"MedEthicsQA","what":"Medical ethics reasoning in LLMs","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Wen-Evaluating Implicit Bias in Large Language Models","year":2025,"benchmark":"BUMBLE (Bias Understanding Measurement Benchmark for LLM Eva","what":"Implicit bias in LLMs toward demographic groups, elicited through psychometric a","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2025-Willis-Will Systems of LLM Agents Cooperate An Investigation into a Social","year":2025,"benchmark":"LLM Agent Cooperation (IPD Evolutionary Simulation)","what":"Cooperative tendencies and strategic biases of LLM agent systems in social dilem","category":"Fairness, Bias & Social Norms","availability":"Code available","url":""},{"filename":"2025-Wu-The Staircase of Ethics Multi-step Moral Dilemmas","year":2025,"benchmark":"Multi-step Moral Dilemmas (MMDs)","what":"Evolving moral judgments and value priorities of LLMs across escalating ethical ","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Xia-SafeToolBench","year":2025,"benchmark":"SafeToolBench","what":"Safety of LLM tool utilization in prospective manner (before tool execution)","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Xiang-Comparing Moral Values in Western","year":2025,"benchmark":"LLM Moral Word Association Framework","what":"Implicit moral values in LLMs compared to Western English-speaking communities u","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Xie-SORRY-Bench - Systematically Evaluating LLM Safety Refusal","year":2025,"benchmark":"SORRY-Bench","what":"LLM safety refusal behavior across fine-grained unsafe topics including ethical ","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2025-Yao-Value Compass Benchmarks","year":2025,"benchmark":"Value Compass Benchmarks","what":"LLM values across 27 value dimensions (expanded from Schwartz Basic Human Values","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Ye-Large Language Model Psychometrics","year":2025,"benchmark":"LLM Psychometrics Framework (survey/meta-analysis)","what":"Personality, values, and intelligence of LLMs using psychometric instruments","category":"Value Alignment & Value Pluralism","availability":"Code available","url":""},{"filename":"2025-Yuan-Probabilistic Aggregation and Targeted Embedding","year":2025,"benchmark":"Probabilistic Aggregation for Collective Moral Reasoning","what":"Moral reasoning consistency and alignment across multiple LLMs; aggregates conti","category":"Moral Reasoning & Ethical Judgment","availability":"Code available","url":""},{"filename":"2025-Yuan-S-Eval","year":2025,"benchmark":"S-Eval","what":"LLM safety across 8 risk dimensions and 102 subdivided risks including ethical v","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Zaim Bin Ahmad-Large-scale moral machine experiment on large","year":2025,"benchmark":"Large-scale Moral Machine Experiment on LLMs","what":"Moral decision-making alignment between LLMs and human moral preferences in trol","category":"Moral Reasoning & Ethical Judgment","availability":"Code available","url":""},{"filename":"2025-Zhang-LLMEval-Med Medical Safety & Ethics Component","year":2025,"benchmark":"LLMEval-Med (Safety & Ethics Component)","what":"Medical LLM safety and ethics alongside medical knowledge, language understandin","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Zhang-REVAL","year":2025,"benchmark":"REVAL","what":"Reliability (truthfulness, robustness) and Values (ethics, safety, privacy) of L","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Zhou-Fair-PP Synthetic Dataset for Aligning LLM with Personalized","year":2025,"benchmark":"Fair-PP","what":"Personalized preferences targeting social equity across 28 social groups, 98 equ","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2025-Zhou-Moral Reasoning Across Languages","year":2025,"benchmark":"Multilingual Moral Reasoning Benchmark (MMRB)","what":"Moral reasoning abilities of LLMs across five typologically diverse languages in","category":"Moral Reasoning & Ethical Judgment","availability":"Code available","url":""},{"filename":"2025-Zhou-SocialEval Evaluating Social Intelligence of Large Language Models","year":2025,"benchmark":"SocialEval","what":"Social intelligence of LLMs including prosociality and social behavior in naviga","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Zhu-EAVIT","year":2025,"benchmark":"EAVIT (Efficient and Accurate Value Identification from Text","what":"Human value identification from text data using Schwartz value theory","category":"Value Alignment & Value Pluralism","availability":"Code available","url":""},{"filename":"2025-Zhu-TrolleyBench","year":2025,"benchmark":"TrolleyBench (duplicate entry)","what":"Ethical consistency and moral reasoning in LLMs using trolley-problem dilemmas","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Zhu-TrolleyBench - Evaluating Emergent Moral Reasoning and Consistency in","year":2025,"benchmark":"TrolleyBench","what":"Ethical consistency and moral reasoning in LLMs using trolley-problem-style mora","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2025-Zohny-ADEPT Simulating Ethics","year":2025,"benchmark":"ADEPT (AI Debate Ethics Panel Tool)","what":"Multi-perspective ethical deliberation in LLMs; how panel composition of ethical","category":"Moral Reasoning & Ethical Judgment","availability":"Code available","url":""},{"filename":"2024-AlKhamissi-Investigating Cultural Alignment of LLMs","year":2024,"benchmark":"Cultural Alignment Survey Replication / Anthropological Prom","what":"Cultural alignment of LLMs with Egyptian and US cultural values, measured throug","category":"Value Alignment & Value Pluralism","availability":"Code available","url":"https://github.com/bkhmsi/cultural-trends.git"},{"filename":"2024-Arshad-HateInsights","year":2024,"benchmark":"HateInsights","what":"Hate speech detection with explainability in Urdu, measuring both classification","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Arshad-Understanding hate speech","year":2024,"benchmark":"HateInsights","what":"Hate speech detection with explainability in Urdu; model plausibility and faithf","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Bhardwaj-Language Models are Homer Simpson! Safety","year":2024,"benchmark":"Multilingual Safety Benchmark (RESTA)","what":"LLM safety after fine-tuning compromise; harmfulness across 11 safety categories","category":"Cultural & Cross-Cultural Ethics","availability":"Code available","url":"https://github.com/declare-lab/resta"},{"filename":"2024-Bonagiri-SaGE","year":2024,"benchmark":"SaGE / Moral Consistency Corpus (MCC)","what":"Moral consistency of LLM generations using information-theoretic measure based o","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":"https://github.com/vnnm404/SaGE"},{"filename":"2024-Bonagiri-SaGE Semantic Graph Entropy Moral Consistency Corpus","year":2024,"benchmark":"SaGE / Moral Consistency Corpus (MCC)","what":"Moral consistency of LLMs in conversational responses using Rules of Thumb (RoTs","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":"https://github.com/vnnm404/SaGE"},{"filename":"2024-Buyl-Large Language Models Reflect the Ideology of","year":2024,"benchmark":"LLM Ideology Analysis","what":"Ideological and normative positions of LLMs from different geopolitical regions;","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Chao-JailbreakBench","year":2024,"benchmark":"JailbreakBench","what":"LLM robustness against jailbreak attacks; harmful/unethical content generation a","category":"Safety & Red Teaming","availability":"Public","url":""},{"filename":"2024-Chao-JailbreakBench - An Open Robustness Benchmark","year":2024,"benchmark":"JailbreakBench","what":"LLM robustness against jailbreak attacks; 100 behaviors aligned with OpenAI usag","category":"Safety & Red Teaming","availability":"Public","url":""},{"filename":"2024-Chao-JBB-Behaviours","year":2024,"benchmark":"JailbreakBench / JBB-Behaviors","what":"LLM robustness against jailbreak attacks across 100 harmful/unethical behaviors ","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2024-Chern-BeHonest","year":2024,"benchmark":"BeHonest","what":"Honesty in LLMs: awareness of knowledge boundaries, avoidance of deceit, and con","category":"Value Alignment & Value Pluralism","availability":"Public","url":"https://github.com/GAIR-NLP/BeHonest"},{"filename":"2024-Chiu-DailyDilemmas","year":2024,"benchmark":"DailyDilemmas","what":"LLM value preferences when navigating everyday moral dilemmas, analyzed through ","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Dorn-BELLS A Framework Towards Future Proof Benchmarks for the Evaluation","year":2024,"benchmark":"BELLS","what":"LLM safeguard effectiveness across failure modes including harmful behavior, adv","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":"https://github.com/CentreSecuriteIA/BELLS"},{"filename":"2024-Duan-DeNEVIL MoralPrompt","year":2024,"benchmark":"MoralPrompt (with DeNEVIL algorithm)","what":"Intrinsic ethical values of LLMs based on moral philosophy value theories; explo","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Fenogenova-MERA A Comprehensive LLM Evaluation in Russian","year":2024,"benchmark":"MERA (ruEthics subtask)","what":"Russian-language LLM evaluation including ruEthics subtask measuring models' und","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":"https://mera.a-ai.ru/en"},{"filename":"2024-Franken-OffTheRails Procedural Dilemma Generation for Moral Reasoning","year":2024,"benchmark":"OffTheRails","what":"Moral permissibility and intention judgments in trolley-problem-style dilemmas (","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Ghosh-AEGIS","year":2024,"benchmark":"AEGIS Safety Dataset","what":"Content safety risks across 13 critical risk categories and 9 sparse risk catego","category":"Safety & Red Teaming","availability":"Public","url":""},{"filename":"2024-Group-The AI Safety Benchmark","year":2024,"benchmark":"AI Safety Benchmark v0.5 (MLCommons)","what":"Safety risks of chat-tuned LLMs across 13 hazard categories including harmful co","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2024-Guldimann-COMPL-AI EU AI Act Compliance","year":2024,"benchmark":"COMPL-AI","what":"EU AI Act compliance of LLMs across six ethical principles: robustness, safety, ","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":"https://github.com/compl-ai/compl-ai"},{"filename":"2024-Gupta-WalledEval","year":2024,"benchmark":"WalledEval (including SGXSTest and HIXSTest)","what":"AI safety across 35+ benchmarks covering multilingual safety, exaggerated safety","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":"https://github.com/walledai/walledeval"},{"filename":"2024-Han-MedSafetyBench","year":2024,"benchmark":"MedSafetyBench","what":"Medical safety of LLMs based on AMA Principles of Medical Ethics (beneficence, n","category":"Safety & Red Teaming","availability":"Public","url":"https://github.com/AI4LIFE-GROUP/med-safety-bench"},{"filename":"2024-Han-WildGuard WildGuardTest","year":2024,"benchmark":"WildGuardTest (with WildGuardMix dataset)","what":"LLM safety moderation: malicious intent detection, safety risk detection in resp","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":"https://huggingface.co/datasets/allenai/wildguardmix"},{"filename":"2024-Hancox-Li-Is ETHICS about ethics","year":2024,"benchmark":"Unnamed (critique of ETHICS benchmark)","what":"Validity of the ETHICS benchmark for testing genuine ethical capabilities of LLM","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Hu-VIVA A Benchmark for Vision-Grounded Decision-Making with Human Values","year":2024,"benchmark":"VIVA","what":"Vision-language models' capacity to leverage human values for decision-making in","category":"Value Alignment & Value Pluralism","availability":"Public","url":"https://github.com/Derekkk/VIVA_EMNLP24"},{"filename":"2024-Huang-CBBQ","year":2024,"benchmark":"CBBQ (Chinese Bias Benchmark for QA)","what":"Societal biases of LLMs across 14 social dimensions related to Chinese culture a","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":"https://anonymous.4open.science/r/CBBQ-B860/"},{"filename":"2024-Huang-Collective Constitutional AI Aligning a Language Model with Public","year":2024,"benchmark":"Collective Constitutional AI (CCAI)","what":"Value alignment of LLMs using collectively-sourced public principles; evaluates ","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Huang-Moral Persuasion in Large Language Models","year":2024,"benchmark":"Unnamed (Moral Persuasion Evaluation)","what":"LLM susceptibility to moral persuasion across ethical frameworks (utilitarian, d","category":"Moral Reasoning & Ethical Judgment","availability":"Code available","url":"https://github.com/acyhuang/moral-persuasion"},{"filename":"2024-Huang-TrustLLM","year":2024,"benchmark":"TrustLLM","what":"LLM trustworthiness across 8 dimensions: truthfulness, safety, fairness, robustn","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Jain-PolygloToxicityPrompts","year":2024,"benchmark":"PolygloToxicityPrompts (PTP)","what":"Multilingual toxicity generation in LLMs across 17 languages","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Ji-MoralBench","year":2024,"benchmark":"MoralBench","what":"Moral identity and moral reasoning capabilities of LLMs across ethical dilemmas ","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Jiang-Can Language Models Reason about Individualistic","year":2024,"benchmark":"IndieValueCatalog","what":"Individualistic value reasoning - whether LLMs can predict a specific individual","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Jiang-WildTeaming at Scale","year":2024,"benchmark":"WildJailbreak","what":"LLM safety and jailbreak resistance; adversarial robustness against real-world a","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2024-Jin-Language Model Alignment in Multilingual Trolley","year":2024,"benchmark":"MultiTP","what":"Cross-lingual moral alignment of LLMs via trolley problems; preferences across 6","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":"https://github.com/causalNLP/moralmachine"},{"filename":"2024-Jin-MULTITP - Language Model Alignment in Multilingual Trolley Problems","year":2024,"benchmark":"MultiTP","what":"Cross-lingual moral alignment of LLMs via trolley problems; moral dimensions: sp","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":"https://github.com/causalNLP/moralmachine"},{"filename":"2024-Kaiyom-HELM Safety","year":2024,"benchmark":"HELM Safety v1.0","what":"Comprehensive LLM safety across risk categories: violence, fraud, discrimination","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":"https://github.com/stanford-crfm/helm"},{"filename":"2024-Kaneko-Eagle","year":2024,"benchmark":"Eagle","what":"Ethical problems in LLM interactions: social bias, opinion bias, toxic language,","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":"https://huggingface.co/datasets/MasahiroKaneko/eagle"},{"filename":"2024-Kaneko-Eagle Ethical Dataset Given from Real Interactions","year":2024,"benchmark":"Eagle","what":"Real-world ethical problems in LLM interactions: social bias, opinion bias, toxi","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":"https://huggingface.co/datasets/MasahiroKaneko/eagle"},{"filename":"2024-Khandelwal-Indian-BhED - A Dataset for Measuring India-Centric Biases in LLMs","year":2024,"benchmark":"Indian-BhED","what":"India-centric stereotypical biases in LLMs, specifically caste and religious ste","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Kirgis-Differences in Moral Foundations of LLMs","year":2024,"benchmark":"Unnamed (MFT-based LLM Moral Foundations evaluation)","what":"Moral foundations of LLMs (care, fairness, loyalty, authority, sanctity, liberty","category":"Moral Reasoning & Ethical Judgment","availability":"Code available","url":"https://github.com/peterkirgis/llm-moral-foundations"},{"filename":"2024-Kirk-PRISM Alignment Dataset","year":2024,"benchmark":"PRISM","what":"Subjective and multicultural alignment of LLMs; value-laden and controversial to","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Kirk-The PRISM Alignment Project","year":2024,"benchmark":"PRISM","what":"Subjective and multicultural LLM alignment on value-laden topics","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Lee-KorNAT","year":2024,"benchmark":"KorNAT","what":"National alignment with South Korean social values and common knowledge","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Li-Benchmarking Ethics in Text-to-Image Models","year":2024,"benchmark":"T2IEthics","what":"Ethical dimensions of text-to-image (T2I) models: fairness (race, age, gender), ","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Li-OpenEval - Benchmarking Chinese LLMs across Capability Alignment and","year":2024,"benchmark":"OpenEval","what":"Chinese LLM capability, alignment (bias, offensiveness, illegalness), and safety","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Li-SALAD-Bench","year":2024,"benchmark":"SALAD-Bench","what":"LLM safety across hierarchical taxonomy spanning three levels; evaluates LLMs, a","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":"https://github.com/OpenSafetyLab/SALAD-BENCH"},{"filename":"2024-Li-The WMDP Benchmark","year":2024,"benchmark":"WMDP (Weapons of Mass Destruction Proxy)","what":"Hazardous knowledge in LLMs related to biosecurity, cybersecurity, and chemical ","category":"Safety & Red Teaming","availability":"Public","url":"https://wmdp.ai"},{"filename":"2024-Li-Value-Spectrum","year":2024,"benchmark":"Value-Spectrum","what":"Value preferences of Vision-Language Models (VLMs) based on Schwartz's value dim","category":"Value Alignment & Value Pluralism","availability":"Public","url":"https://github.com/Jeremyyny/Value-Spectrum"},{"filename":"2024-Li-WMDP Benchmark Measuring and Reducing Malicious Use With Unlearning","year":2024,"benchmark":"WMDP (Weapons of Mass Destruction Proxy)","what":"Hazardous knowledge in LLMs (biosecurity, cybersecurity, chemical security)","category":"Safety & Red Teaming","availability":"Public","url":"https://wmdp.ai"},{"filename":"2024-Liu-CultureAtlas","year":2024,"benchmark":"CultureAtlas","what":"Cross-cultural knowledge and cultural commonsense in LLMs, covering 1000+ sub-co","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Liu-JAILJUDGE","year":2024,"benchmark":"JAILJUDGE","what":"LLM safety against jailbreak attacks across synthetic, adversarial, in-the-wild,","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Mazeika-HarmBench","year":2024,"benchmark":"HarmBench","what":"LLM robustness against automated red teaming attacks designed to elicit harmful ","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2024-MLCommons-MLCommons AILuminate Benchmark","year":2024,"benchmark":"AILuminate v1.0","what":"AI system resistance to prompts designed to elicit dangerous, illegal, or undesi","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2024-Mou-SG-Bench","year":2024,"benchmark":"SG-Bench","what":"LLM safety generalization across diverse tasks (generative and discriminative) a","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2024-Mozikov-EAI - Emotional Decision-Making of LLMs in Strategic Games and","year":2024,"benchmark":"EAI (Emotional AI Framework)","what":"Emotional impact on LLM ethical decision-making in strategic games and ethical d","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Obi-Value Imprint","year":2024,"benchmark":"Value Imprint","what":"Human values embedded within RLHF datasets used for LLM alignment, using a taxon","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Ohashi-Extended Japanese Commonsense Morality Dataset","year":2024,"benchmark":"eJCM (Extended Japanese Commonsense Morality)","what":"Japanese commonsense morality classification, including culturally-specific mora","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Piatti-Cooperate or Collapse","year":2024,"benchmark":"GovSim (Governance of the Commons Simulation)","what":"LLM cooperative decision-making, ethical reasoning, strategic planning, and abil","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Pistilli-CIVICS","year":2024,"benchmark":"CIVICS (Culturally-Informed and Values-Inclusive Corpus for ","what":"Cultural and social value variation in LLMs across languages, nations, and sensi","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Qiu-ProgressGym","year":2024,"benchmark":"ProgressGym","what":"LLM ability to track evolving moral values, anticipate moral progress, and regul","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Ren-Safetywashing","year":2024,"benchmark":"Safetywashing Meta-Analysis","what":"Correlation between AI safety benchmarks and general capabilities/compute, ident","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2024-Ren-Safetywashing Do AI Safety Benchmarks Actually Measure Safety Progress","year":2024,"benchmark":"Safetywashing Meta-Analysis Framework","what":"Whether AI safety benchmarks (including machine ethics, bias, alignment) actuall","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Ren-ValueBench","year":2024,"benchmark":"ValueBench","what":"Value orientations and value understanding in LLMs using established psychometri","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Rottger-Political Compass or Spinning Arrow","year":2024,"benchmark":"Political Compass Test (PCT) evaluation framework","what":"Values and opinions in LLMs, particularly political biases and the robustness/re","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Rottger-SafetyPrompts Systematic Review of Open Datasets for LLM Safety","year":2024,"benchmark":"SafetyPrompts","what":"Comprehensive catalogue of LLM safety evaluation datasets covering bias, toxicit","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2024-Rozado-The Political Preferences of LLMs","year":2024,"benchmark":"Political Orientation Test Battery","what":"Political preferences and biases embedded in LLMs across multiple ideological di","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Ryan-Unintended Impacts of LLM Alignment on Global Representation","year":2024,"benchmark":"Global Representation Evaluation","what":"Impact of LLM alignment (RLHF/DPO) on global representation across English diale","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Röttger-Political Compass or Spinning Arrow","year":2024,"benchmark":"Political Compass Test (PCT) evaluation framework","what":"Values and opinions in LLMs via the Political Compass Test, examining robustness","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Röttger-XSTest","year":2024,"benchmark":"XSTest","what":"Exaggerated safety behaviors (over-refusals) in LLMs, measuring the safety-helpf","category":"Safety & Red Teaming","availability":"Public","url":""},{"filename":"2024-Saffari-PSN","year":2024,"benchmark":"ISN (Iranian Social Norms)","what":"LLM comprehension of Iranian social norms, testing cross-cultural understanding ","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Sahoo-IndiBias - Benchmark Dataset for Social Biases in Indian Context","year":2024,"benchmark":"IndiBias","what":"Social biases in LLMs within the Indian cultural context across gender, religion","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Senthilkumar-Fine-Tuning Language Models for Ethical Ambiguity","year":2024,"benchmark":"Scruples (DILEMMAS and ANECDOTES)","what":"LLM calibration on morally ambiguous scenarios, comparing model probability dist","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Sharma-Towards Understanding Sycophancy in Language Models","year":2024,"benchmark":"Sycophancy Evaluation","what":"Sycophancy behavior in AI assistants - tendency to match user beliefs over truth","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Singhal-AyaRedTeaming","year":2024,"benchmark":"AyaRedTeaming","what":"Multilingual safety alignment distinguishing global harms (universal across cult","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Sorensen-A Roadmap to Pluralistic Alignment","year":2024,"benchmark":"Pluralistic Alignment Benchmark Framework","what":"Pluralism in AI value alignment - whether models can represent diverse values (O","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Sorensen-Value Kaleidoscope","year":2024,"benchmark":"ValuePrism / Value Kaleidoscope (Kaleido)","what":"Pluralistic human values, rights, and duties in context; ability to model value ","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Sorensen-Value Kaleidoscope ValuePrism","year":2024,"benchmark":"ValuePrism / Value Kaleidoscope (Kaleido)","what":"Pluralistic human values, rights, and duties contextualized in real-world situat","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Souly-StrongREJECT","year":2024,"benchmark":"StrongREJECT","what":"Jailbreak effectiveness and LLM safety robustness; measures whether jailbreak at","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2024-Takemoto-The moral machine experiment on large language","year":2024,"benchmark":"Moral Machine Experiment for LLMs","what":"Ethical decision-making tendencies of LLMs on trolley-problem-style autonomous v","category":"Moral Reasoning & Ethical Judgment","availability":"Code available","url":""},{"filename":"2024-Team-FABLE Fairness and Bias in LLM Evaluation","year":2024,"benchmark":"FACT-OR-FAIR","what":"Fairness and bias in LLMs, distinguishing between factual correctness and normat","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2024-Team-GuardBench","year":2024,"benchmark":"GuardBench","what":"Safety of guardrail models (input-output filters) for LLMs, measuring ability to","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Team-WildGuard","year":2024,"benchmark":"WildGuard / WildGuardMix / WildGuardTest","what":"LLM safety across 13 risk categories: malicious intent detection, safety risk de","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Tian-Chinese SafetyQA - A Safety Short-form Factuality Benchmark","year":2024,"benchmark":"Chinese SafetyQA","what":"LLM factuality in safety-critical domains including law, policy, and ethics in C","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Tom-SG-Bench - Evaluating LLM Safety Generalization","year":2024,"benchmark":"SG-Bench","what":"LLM safety generalization across discriminative and generative evaluation paradi","category":"Fairness, Bias & Social Norms","availability":"Public","url":"https://github.com/MurrayTom/SG-Bench"},{"filename":"2024-Various-SGBench","year":2024,"benchmark":"SG-Bench","what":"LLM safety generalization across diverse tasks and prompt types (discriminative ","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2024-Wang-All Languages Matter","year":2024,"benchmark":"XSafety","what":"Multilingual safety of LLMs across 14 safety issues in 10 languages","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Wang-Do-Not-Answer","year":2024,"benchmark":"Do-Not-Answer","what":"Safeguards in LLMs -- evaluates ability to refuse responding to dangerous/harmfu","category":"Safety & Red Teaming","availability":"Public","url":"https://github.com/Libr-AI/do-not-answer"},{"filename":"2024-Wu-WorldValuesBench","year":2024,"benchmark":"WorldValuesBench","what":"Multi-cultural value awareness of language models across diverse global demograp","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Yan-M$^3$oralBench","year":2024,"benchmark":"M3oralBench","what":"Multimodal moral understanding and reasoning of large vision-language models acr","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Yanaka-JBBQ","year":2024,"benchmark":"JBBQ (Japanese Bias Benchmark for Question Answering)","what":"Social biases in Japanese LLMs (same benchmark as the companion paper)","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":"https://github.com/ynklab/JBBQ_data"},{"filename":"2024-Yanaka-JBBQ Japanese Bias Benchmark for Question Answering","year":2024,"benchmark":"JBBQ (Japanese Bias Benchmark for Question Answering)","what":"Social biases in Japanese LLMs across categories of age, disability, gender, phy","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":"https://github.com/ynklab/JBBQ_data"},{"filename":"2024-Yin-SafeAgentBench","year":2024,"benchmark":"SafeAgentBench","what":"Safety awareness of embodied LLM agents in interactive simulation environments, ","category":"Safety & Red Teaming","availability":"Public","url":"https://github.com/shengyin1224/SafeAgentBench"},{"filename":"2024-Yu-CMoralEval","year":2024,"benchmark":"CMoralEval","what":"Moral alignment of Chinese LLMs across Chinese-specific moral norms (familial, s","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2024-Yu-CoSafe","year":2024,"benchmark":"CoSafe","what":"LLM safety in multi-turn dialogue coreference -- evaluates whether models can be","category":"Safety & Red Teaming","availability":"Public","url":"https://github.com/ErxinYu/CoSafe-Dataset"},{"filename":"2024-Yuan-R-Judge","year":2024,"benchmark":"R-Judge","what":"Behavioral safety risk awareness of LLM agents in interactive environments acros","category":"Safety & Red Teaming","availability":"Public","url":"https://github.com/Lordog/R-Judge"},{"filename":"2024-Yuan-Social Measuring Social Norms of LLMs","year":2024,"benchmark":"Social (Social Norms Dataset)","what":"LLMs' understanding of social norms covering opinions, arguments, culture, and l","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":"https://github.com/socialnormdataset/socialagent"},{"filename":"2024-Zeng-AIR-Bench 2024","year":2024,"benchmark":"AIR-Bench 2024","what":"AI safety aligned with government regulations and company policies across 314 gr","category":"Safety & Red Teaming","availability":"Public","url":""},{"filename":"2024-Zhang-Agent-SafetyBench Evaluating the Safety of LLM Agents","year":2024,"benchmark":"Agent-SafetyBench","what":"Safety of LLM agents across 8 categories of safety risks and 10 common failure m","category":"Safety & Red Teaming","availability":"Public","url":"https://github.com/thu-coai/Agent-SafetyBench"},{"filename":"2024-Zhang-ChineseSafe - A Chinese Benchmark for Evaluating Safety in LLMs","year":2024,"benchmark":"ChineseSafe","what":"Content safety of LLMs in Chinese contexts across 4 classes and 10 sub-classes i","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Zhang-CHiSafetyBench","year":2024,"benchmark":"CHiSafetyBench","what":"Chinese-language safety: risky content identification and refusal to answer risk","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Zhang-CHiSafetyBench - A Chinese Hierarchical Safety Benchmark for LLMs","year":2024,"benchmark":"CHiSafetyBench","what":"Chinese LLM safety across 5 risk areas and 31 categories in a hierarchical Chine","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":"https://github.com/UnicomAI/UnicomBenchmark/tree/main/CHiSafetyBench"},{"filename":"2024-Zhang-SafetyBench","year":2024,"benchmark":"SafetyBench","what":"LLM safety across 7 categories (offense, ethics, physical harm, etc.) in Chinese","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Zhang-SafetyBench Evaluating the Safety of Large Language Models","year":2024,"benchmark":"SafetyBench","what":"LLM safety across 7 distinct categories of safety concerns in both Chinese and E","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2024-Zhang-ValueDCG","year":2024,"benchmark":"ValueDCG","what":"LLMs' comprehensive understanding of human values, distinguishing 'know what' (s","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2024-Zheng-ALI-Agent - Assessing LLMs' Alignment with Human Values via","year":2024,"benchmark":"ALI-Agent","what":"LLM alignment with human values across stereotypes, morality, and legality","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2023-Aroyo-DICES Dataset","year":2023,"benchmark":"DICES (Diversity In Conversational AI Evaluation for Safety)","what":"Safety evaluation of conversational AI with demographic diversity - how safety p","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2023-Atre-Benchmarking Cognitive Biases in Large Language","year":2023,"benchmark":"CoBBLEr (Cognitive Bias Benchmark for LLMs as Evaluators)","what":"Cognitive biases in LLMs when used as evaluators - 6 cognitive biases including ","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":"https://minnesotanlp.github.io/cobbler"},{"filename":"2023-Ayele-Amharic Hate Speech Dataset","year":2023,"benchmark":"Amharic Hate Speech Dataset","what":"Hate speech detection in Amharic (Ethiopian language) - identifying hateful cont","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2023-Dai-Safe RLHF","year":2023,"benchmark":"BeaverTails (used) / Safe RLHF evaluation","what":"Helpfulness vs. harmlessness trade-off in LLM responses; safety alignment measur","category":"Safety & Red Teaming","availability":"Public","url":""},{"filename":"2023-Duan-Denevil","year":2023,"benchmark":"MoralPrompt (via DeNEVIL)","what":"Intrinsic ethical values of LLMs from a moral philosophy perspective; value comp","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":"https://valuecompass.github.io"},{"filename":"2023-Durmus-Towards Measuring the Representation of","year":2023,"benchmark":"GlobalOpinionQA","what":"Representation of subjective global opinions in LLMs; whose values/opinions LLM ","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2023-Fischer-What does ChatGPT return about human values","year":2023,"benchmark":"Unnamed (Schwartz Value Theory probes for ChatGPT)","what":"Value biases in ChatGPT using Schwartz's Basic Value Theory; tests if LLM output","category":"Value Alignment & Value Pluralism","availability":"Public","url":"https://osf.io/w46nq/"},{"filename":"2023-Fleisig-FairPrism","year":2023,"benchmark":"FairPrism","what":"Fairness-related harms in AI text generation: stereotyping and demeaning harms r","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2023-Huang-Flames","year":2023,"benchmark":"FLAMES","what":"Value alignment of LLMs in Chinese; covers harmlessness, safety, fairness, and a","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":"https://github.com/AIFlames/Flames"},{"filename":"2023-Ji-BeaverTails","year":2023,"benchmark":"BeaverTails","what":"Safety alignment in LLMs; separates helpfulness and harmlessness annotations for","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2023-Jiang-MACHIAVELLI","year":2023,"benchmark":"MACHIAVELLI","what":"Ethical behavior in AI agents; measures trade-offs between reward maximization a","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2023-Lin-ToxicChat","year":2023,"benchmark":"ToxicChat","what":"Toxicity in real-world user-AI conversations; nuanced toxic phenomena distinct f","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2023-Liu-AlignBench","year":2023,"benchmark":"AlignBench","what":"Chinese LLM alignment across multiple dimensions including reasoning, language u","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2023-Miah-Islam QA","year":2023,"benchmark":"Islam QA Dataset","what":"QA system performance on Islamic religious texts (Quranic Tafsir and Ahadith), i","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2023-Mougan-Kantian Deontology Meets AI Alignment","year":2023,"benchmark":"Unnamed","what":"Ethical grounding of AI fairness metrics, analyzing whether existing fairness me","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2023-Nie-MoCa","year":2023,"benchmark":"MoCa","what":"Human-LLM alignment on causal and moral judgment tasks, testing factors like nor","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2023-Pan-Do the Rewards Justify the Means Measuring","year":2023,"benchmark":"MACHIAVELLI","what":"Trade-offs between reward maximization and ethical behavior in AI agents; measur","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2023-Qian-Understanding Chinese Moral Stories with Further","year":2023,"benchmark":"STORAL-ZH","what":"AI model ability to understand moral concepts embedded in Chinese narrative stor","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2023-Rao-What Makes it Ok to Set a Fire Iterative","year":2023,"benchmark":"delta-Rules-of-Thumb (delta-RoT)","what":"Defeasible moral reasoning - context-dependent moral acceptability of actions wi","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2023-Santurkar-OpinionQA","year":2023,"benchmark":"OpinionQA","what":"Alignment of LLM opinions with 60 US demographic groups on topics including abor","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2023-Santurkar-Whose Opinions Do Language Models Reflect","year":2023,"benchmark":"OpinionQA","what":"Alignment of LLM opinions with 60 US demographic groups across topics ranging fr","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2023-Scherrer-Evaluating the Moral Beliefs Encoded in LLMs","year":2023,"benchmark":"MoralChoice","what":"Moral beliefs encoded in LLMs, measuring consistency and uncertainty of moral ch","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2023-Shen-“Do Anything Now”","year":2023,"benchmark":"JailbreakHub","what":"LLM vulnerability to jailbreak attacks across 13 forbidden scenarios (safety/eth","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2023-Tanmay-Probing Moral Development through Defining Issues Test DIT","year":2023,"benchmark":"DIT (Defining Issues Test) for LLMs","what":"Moral development stage of LLMs according to Kohlberg's Cognitive Moral Developm","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2023-Wang-DecodingTrust","year":2023,"benchmark":"DecodingTrust","what":"Comprehensive trustworthiness of GPT models across 8 dimensions: toxicity, stere","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2023-Wang-STREAM","year":2023,"benchmark":"STREAM","what":"AI model alignment with dynamically evolving human moral values through collecti","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2023-Xu-CValues","year":2023,"benchmark":"CValues","what":"Chinese LLM alignment with human values across safety (10 scenarios) and respons","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2023-Xu-CValues - Measuring the Values of Chinese LLMs from Safety to","year":2023,"benchmark":"CValues","what":"Chinese LLM alignment with human values in terms of safety (10 scenarios) and re","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2023-Xu-SC-Safety","year":2023,"benchmark":"SuperCLUE-Safety (SC-Safety)","what":"Safety of Chinese LLMs across 20+ safety sub-dimensions including ethical violat","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2023-Yao-Value FULCRA","year":2023,"benchmark":"FULCRA","what":"LLM behavior alignment with Schwartz's Theory of Basic Human Values; maps LLM ou","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2023-Ziems-NormBank","year":2023,"benchmark":"NormBank","what":"Situational social norms grounded in sociocultural frames (setting, roles, attri","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2022-Bai-HH-RLHF Helpful and Harmless RLHF","year":2022,"benchmark":"HH-RLHF (Helpful and Harmless RLHF Dataset)","what":"Helpfulness and harmlessness of language model assistants via human preference c","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2022-Bai-Training a Helpful and Harmless Assistant with","year":2022,"benchmark":"HH-RLHF (Helpful and Harmless RLHF Dataset)","what":"Helpfulness and harmlessness of language model assistants via human preference c","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2022-Ganguli-Red Teaming Language Models to Reduce Harms","year":2022,"benchmark":"Anthropic Red Team Dataset","what":"Harmful outputs of language models including offensive language, unethical outpu","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2022-Hartvigsen-ToxiGen","year":2022,"benchmark":"ToxiGen","what":"Implicit and explicit toxicity toward 13 minority demographic groups; model abil","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2022-Hendrycks-What Would Jiminy Cricket Do","year":2022,"benchmark":"Jiminy Cricket","what":"Moral behavior of AI agents in semantically rich environments; whether agents ca","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2022-Jeong-Zero-shot Visual Commonsense Immorality Prediction","year":2022,"benchmark":"Visual Commonsense Immorality Benchmark","what":"Visual commonsense immorality - predicting whether images depict morally problem","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2022-Jiang-Can Machines Learn Morality","year":2022,"benchmark":"Commonsense Norm Bank / Delphi","what":"Commonsense moral judgments across diverse social situations - whether actions a","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2022-Kim-ProsocialDialog","year":2022,"benchmark":"ProsocialDialog","what":"Dialogue safety and prosocial response generation - ability of agents to respond","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2022-Kim-ProsocialDialog A Prosocial Backbone for Conversational Agents","year":2022,"benchmark":"ProsocialDialog","what":"Ability of dialogue agents to respond to problematic content (unethical, biased,","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2022-Liang-Holistic Evaluation of Language Models","year":2022,"benchmark":"HELM (Holistic Evaluation of Language Models)","what":"Comprehensive LM evaluation including fairness, bias, toxicity alongside accurac","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2022-Parrish-BBQ","year":2022,"benchmark":"BBQ (Bias Benchmark for QA)","what":"Social biases in QA models against protected classes along 9 social dimensions i","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2022-Parrish-BBQ Bias Benchmark for Question Answering","year":2022,"benchmark":"BBQ (Bias Benchmark for QA)","what":"Social biases in QA models against protected classes across 9 social dimensions ","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2022-Qiu-ValueNet","year":2022,"benchmark":"ValueNet","what":"Human values across 10 Schwartz-based value dimensions in text scenarios, for va","category":"Value Alignment & Value Pluralism","availability":"Public","url":"https://liang-qiu.github.io/ValueNet/"},{"filename":"2022-Röttger-Multilingual HateCheck","year":2022,"benchmark":"Multilingual HateCheck (MHC)","what":"Hate speech detection model capabilities across 10 languages via functional test","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2022-Schramowski-I2P Inappropriate Image Prompts Test Bed","year":2022,"benchmark":"I2P (Inappropriate Image Prompts) / Q16","what":"Inappropriate content in image datasets and image generation models - offensive,","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2022-Trager-The Moral Foundations Reddit Corpus","year":2022,"benchmark":"MFRC (Moral Foundations Reddit Corpus)","what":"Moral sentiment in text based on updated Moral Foundations Theory - 8 categories","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2022-Ziems-Moral Integrity Corpus MIC","year":2022,"benchmark":"MIC (Moral Integrity Corpus)","what":"Moral assumptions and integrity of conversational agents - whether chatbot repli","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2022-Ziems-The Moral Integrity Corpus","year":2022,"benchmark":"MIC (Moral Integrity Corpus)","what":"Moral integrity of dialogue systems - moral assumptions reflected in chatbot utt","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2021-Barikeri-REDDITBIAS","year":2021,"benchmark":"REDDITBIAS","what":"Social bias in conversational language models across gender, race, religion, and","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2021-Dhamala-BOLD","year":2021,"benchmark":"BOLD (Bias in Open-Ended Language Generation Dataset)","what":"Social biases in open-ended text generation across profession, gender, race, rel","category":"Value Alignment & Value Pluralism","availability":"Public","url":""},{"filename":"2021-Emelin-Moral Stories","year":2021,"benchmark":"Moral Stories","what":"Grounded, goal-oriented social reasoning and norm adherence in language models; ","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2021-Hendrycks-Aligning AI with Shared Human Values","year":2021,"benchmark":"ETHICS","what":"Language model knowledge of basic moral concepts spanning justice, well-being, d","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2021-Lourie-Scruples","year":2021,"benchmark":"SCRUPLES","what":"Descriptive ethics and moral judgment in real-life interpersonal situations; com","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2021-Munshi-Towards an Automated Islamic Fatwa System","year":2021,"benchmark":"Islamic Fatwa Dataset","what":"Islamic jurisprudence (Sharia) question-answering and topic classification for f","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2021-Nadeem-StereoSet","year":2021,"benchmark":"StereoSet","what":"Stereotypical bias in pretrained language models across gender, profession, race","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2021-Nangia-CrowS-Pairs","year":2021,"benchmark":"CrowS-Pairs","what":"Social bias in masked language models against protected demographic groups acros","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2021-Röttger-HateCheck","year":2021,"benchmark":"HateCheck","what":"Functional weaknesses of hate speech detection models across 29 specific functio","category":"Toxicity, Hate Speech & Harmful Content","availability":"Public","url":""},{"filename":"2020-Awad-Universals and variations in moral decisions made","year":2020,"benchmark":"Moral Machine Classic Mode Dataset","what":"Cross-cultural moral preferences in sacrificial (trolley-problem) dilemmas; univ","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2020-Forbes-Social Chemistry 101","year":2020,"benchmark":"SOCIAL-CHEM-101","what":"Social norms and moral judgments via rules-of-thumb with 12 annotation dimension","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2020-Forbes-Social Chemistry 101 SOCIAL-CHEM-101","year":2020,"benchmark":"SOCIAL-CHEM-101","what":"Social norms and moral judgments (rules-of-thumb) across 12 dimensions including","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2020-Gehman-RealToxicityPrompts","year":2020,"benchmark":"RealToxicityPrompts","what":"Neural toxic degeneration in pretrained language models (propensity to generate ","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2020-Hoover-Moral Foundations Twitter Corpus","year":2020,"benchmark":"Moral Foundations Twitter Corpus (MFTC)","what":"Moral sentiment in natural language based on Moral Foundations Theory (Care/Harm","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2020-Kumar-Evaluating Aggression Identification in Social","year":2020,"benchmark":"TRAC-2 Shared Task Dataset","what":"Aggression and gendered aggression identification in social media (YouTube comme","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2020-Mathew-HateXplain","year":2020,"benchmark":"HateXplain","what":"Explainable hate speech detection: classification (hate/offensive/normal), targe","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2020-Nangia-CrowS-Pairs","year":2020,"benchmark":"CrowS-Pairs","what":"Social biases in masked language models across 9 bias categories (race, gender, ","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2020-Sap-Social Bias Frames","year":2020,"benchmark":"Social Bias Inference Corpus (SBIC)","what":"Implicit social bias in language, including stereotyping, offensiveness, and the","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2020-Schramowski-The Moral Choice Machine","year":2020,"benchmark":"Moral Choice Machine (MCM)","what":"Deontological ethical reasoning about right and wrong conduct extracted from tex","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2019-Basile-SemEval-2019 Task 5","year":2019,"benchmark":"HatEval (SemEval-2019 Task 5)","what":"Hate speech detection against immigrants and women in English and Spanish tweets","category":"Cultural & Cross-Cultural Ethics","availability":"Public","url":""},{"filename":"2019-Jigsaw-Jigsaw Unintended Bias in Toxicity Classification","year":2019,"benchmark":"ToxicBias / Jigsaw Unintended Bias Dataset","what":"Unintended social bias in toxicity classification (models flagging identity term","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2019-Kreutz-GermEval 2019 Task 2","year":2019,"benchmark":"GermEval 2019 Task 2","what":"Offensive language identification in German tweets (coarse binary, fine-grained ","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2019-Struß-Overview of GermEval Task 2, 2019 Shared Task on","year":2019,"benchmark":"GermEval 2019 Shared Task on Offensive Language","what":"Offensive language identification in German tweets with explicit vs. implicit of","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2018-Awad-The Moral Machine experiment","year":2018,"benchmark":"Moral Machine","what":"Cross-cultural moral preferences for autonomous vehicle ethical dilemmas (trolle","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2018-Behzadan-TrolleyMod v1.0","year":2018,"benchmark":"TrolleyMod v1.0","what":"Ethical decision-making in autonomous vehicle trolley-problem scenarios","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2018-Crone-The Socio-Moral Image Database (SMID)","year":2018,"benchmark":"SMID (Socio-Moral Image Database)","what":"Moral wrongness, affective valence/arousal, and relevance to five Moral Foundati","category":"Moral Reasoning & Ethical Judgment","availability":"Public","url":""},{"filename":"2018-Dixon-Measuring and Mitigating Unintended Bias in Text","year":2018,"benchmark":"Identity Phrase Template Test Set / Pinned AUC","what":"Unintended identity-term bias in toxicity classifiers (models falsely flagging n","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2018-Kiritchenko-Examining Gender and Race Bias in Two Hundred","year":2018,"benchmark":"Equity Evaluation Corpus (EEC)","what":"Gender and race bias in sentiment analysis systems (whether systems assign diffe","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2018-Zhao-Gender Bias in Coreference Resolution","year":2018,"benchmark":"WinoBias","what":"Gender bias in coreference resolution systems (whether systems preferentially li","category":"Fairness, Bias & Social Norms","availability":"Public","url":""},{"filename":"2016-Bolukbasi-Man is to Computer Programmer as Woman is to","year":2016,"benchmark":"Word Embedding Association Test (WEAT) / Debiasing Metrics","what":"Gender bias in word embeddings (stereotypical associations between gender and oc","category":"Fairness, Bias & Social Norms","availability":"Public","url":""}],"topAuthors":[{"name":"Dan Hendrycks","count":4},{"name":"Paul Röttger","count":4},{"name":"Yejin Bang","count":4},{"name":"Liwei Jiang","count":4},{"name":"Yu Ying Chiu","count":4},{"name":"Taylor Sorensen","count":4},{"name":"Elizaveta Tennant","count":4},{"name":"Edmond Awad","count":3},{"name":"Iason Gabriel","count":3},{"name":"Jakob Stenseke","count":3},{"name":"Ashutosh Dwivedi","count":3},{"name":"Jiaming Ji","count":3},{"name":"Jing Yao","count":3},{"name":"Zhaowei Zhang","count":3},{"name":"Shaona Ghosh","count":3}],"categoryNames":["Moral Reasoning & Ethical Judgment","Value Alignment & Value Pluralism","Cultural & Cross-Cultural Ethics","Fairness, Bias & Social Norms","Toxicity, Hate Speech & Harmful Content","Safety & Red Teaming","Domain-Specific Ethics","Normative Ethics Benchmarks","Moral Psychology Applied to AI","Helpfulness, Honesty & RLHF"],"llmModels":[{"name":"GPT-4","count":319},{"name":"Llama-3","count":236},{"name":"Claude","count":183},{"name":"Qwen","count":183},{"name":"GPT-3.5","count":158},{"name":"Mistral","count":157},{"name":"Gemini","count":153},{"name":"Llama-2","count":138},{"name":"DeepSeek","count":109},{"name":"Gemma","count":100},{"name":"ChatGPT","count":65},{"name":"BERT","count":63},{"name":"Mixtral","count":54},{"name":"Vicuna","count":51},{"name":"Llama","count":48},{"name":"RoBERTa","count":46},{"name":"GPT-4o","count":35},{"name":"Phi","count":33},{"name":"GPT-3","count":32},{"name":"Falcon","count":26}],"llmByYear":{"2016":{"assessed":1,"notAssessed":5},"2018":{"assessed":1,"notAssessed":10},"2019":{"assessed":6,"notAssessed":6},"2020":{"assessed":7,"notAssessed":13},"2021":{"assessed":13,"notAssessed":9},"2022":{"assessed":23,"notAssessed":19},"2023":{"assessed":63,"notAssessed":48},"2024":{"assessed":171,"notAssessed":85},"2025":{"assessed":265,"notAssessed":78},"2026":{"assessed":16,"notAssessed":5}},"repoDistribution":[{"name":"GitHub","count":320},{"name":"Other","count":70},{"name":"HuggingFace","count":47},{"name":"OSF","count":11},{"name":"Zenodo","count":3},{"name":"Kaggle","count":1}]};

const CAT_COLORS = [
  '#6366f1','#22d3ee','#f59e0b','#ec4899','#22c55e',
  '#a855f7','#f97316','#14b8a6','#e879f9','#38bdf8'
];

// ── Tab navigation ────────────────────────────────────────────────────
document.querySelectorAll('.tab').forEach(tab => {
  tab.addEventListener('click', () => {
    document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
    document.querySelectorAll('.section').forEach(s => s.classList.remove('active'));
    tab.classList.add('active');
    document.getElementById('sec-' + tab.dataset.tab).classList.add('active');
    // Resize charts on tab switch (ECharts needs this)
    setTimeout(() => window.dispatchEvent(new Event('resize')), 50);
  });
});

// ── KPI count-up animation ────────────────────────────────────────────
function animateValue(el, end, suffix='') {
  const duration = 1000;
  const start = 0;
  const startTime = performance.now();
  function update(now) {
    const elapsed = now - startTime;
    const progress = Math.min(elapsed / duration, 1);
    const eased = 1 - Math.pow(1 - progress, 3);
    const current = Math.round(start + (end - start) * eased);
    el.textContent = current.toLocaleString() + suffix;
    if (progress < 1) requestAnimationFrame(update);
  }
  requestAnimationFrame(update);
}

// ── Build KPI cards ───────────────────────────────────────────────────
(function buildKPIs() {
  const kpis = [
    { value: DATA.kpi.total, label: 'Papers Screened', suffix: '' },
    { value: DATA.kpi.candidates, label: 'Benchmark Candidates', suffix: '' },
    { value: DATA.kpi.public, label: 'Public Benchmarks', suffix: '' },
    { value: DATA.kpi.pluralisticPct, label: 'Pluralistic Relevance', suffix: '%' },
    { value: DATA.kpi.normative, label: 'Normative Ethics', suffix: '' },
    { value: DATA.kpi.llmModels, label: 'Papers with LLM Data', suffix: '' },
  ];
  const row = document.getElementById('kpi-row');
  kpis.forEach(k => {
    const div = document.createElement('div');
    div.className = 'kpi';
    div.innerHTML = '<div class="kpi-value" data-target="' + k.value + '" data-suffix="' + k.suffix + '">0</div>'
                  + '<div class="kpi-label">' + k.label + '</div>';
    row.appendChild(div);
  });
  // Animate after a short delay
  setTimeout(() => {
    row.querySelectorAll('.kpi-value').forEach(el => {
      animateValue(el, parseInt(el.dataset.target), el.dataset.suffix);
    });
  }, 200);
})();

// ── ECharts helpers ───────────────────────────────────────────────────
function initChart(id) {
  const dom = document.getElementById(id);
  if (!dom) return null;
  const chart = echarts.init(dom, null, { renderer: 'canvas' });
  window.addEventListener('resize', () => chart.resize());
  return chart;
}

const commonTooltip = {
  backgroundColor: 'rgba(24,24,27,0.95)',
  borderColor: '#3f3f46',
  textStyle: { color: '#fafafa', fontSize: 12 },
};

// ── 1. Timeline (stacked area) ────────────────────────────────────────
(function() {
  const chart = initChart('chart-timeline');
  if (!chart) return;
  const years = Object.keys(DATA.timeline).map(Number);
  const allVals = years.map(y => DATA.timeline[y].all);
  const candVals = years.map(y => DATA.timeline[y].cand);
  const exclVals = years.map(y => DATA.timeline[y].all - DATA.timeline[y].cand);
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'axis' },
    legend: { top: 0, textStyle: { color: '#a1a1aa', fontSize: 11 }, itemWidth: 14, itemHeight: 10 },
    grid: { left: 50, right: 20, top: 40, bottom: 30 },
    xAxis: { type: 'category', data: years, axisLine: { lineStyle: { color: '#3f3f46' } }, axisLabel: { color: '#71717a' } },
    yAxis: { type: 'value', splitLine: { lineStyle: { color: '#27272a' } }, axisLabel: { color: '#71717a' } },
    series: [
      { name: 'Excluded', type: 'bar', stack: 'total', data: exclVals, itemStyle: { color: '#3f3f46' }, barWidth: '60%' },
      { name: 'Benchmarks', type: 'bar', stack: 'total', data: candVals, itemStyle: { color: '#6366f1' }, barWidth: '60%' },
      { name: 'Benchmark %', type: 'line', data: years.map((y,i) => allVals[i]>0 ? Math.round(candVals[i]/allVals[i]*100) : 0),
        yAxisIndex: 0, symbol: 'circle', symbolSize: 6, lineStyle: { color: '#f59e0b', width: 2 },
        itemStyle: { color: '#f59e0b' }, z: 10 },
    ],
  });
})();

// ── 2. Category bar chart ─────────────────────────────────────────────
(function() {
  const chart = initChart('chart-categories');
  if (!chart) return;
  const cats = DATA.categories.slice().sort((a,b) => a.count - b.count);
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'axis', axisPointer: { type: 'shadow' } },
    grid: { left: 220, right: 40, top: 10, bottom: 10 },
    xAxis: { type: 'value', splitLine: { lineStyle: { color: '#27272a' } }, axisLabel: { color: '#71717a' } },
    yAxis: { type: 'category', data: cats.map(c => c.short), axisLabel: { color: '#a1a1aa', fontSize: 11 }, axisLine: { lineStyle: { color: '#3f3f46' } } },
    series: [{
      type: 'bar', data: cats.map((c,i) => ({ value: c.count, itemStyle: { color: CAT_COLORS[c.id-1] } })),
      barWidth: '65%',
      label: { show: true, position: 'right', color: '#a1a1aa', fontSize: 11, fontFamily: 'JetBrains Mono' },
    }],
  });
})();

// ── 3. Availability donut ─────────────────────────────────────────────
(function() {
  const chart = initChart('chart-availability');
  if (!chart) return;
  const d = DATA.availability;
  const items = [
    { name: 'Public', value: d['Public'] || 0 },
    { name: 'Code available', value: d['Code available'] || 0 },
    { name: 'Partially public', value: d['Partially public'] || 0 },
    { name: 'Not stated', value: d['Not stated'] || 0 },
    { name: 'Not public', value: d['Not public'] || 0 },
  ].filter(x => x.value > 0);
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'item', formatter: '{b}: {c} ({d}%)' },
    legend: { bottom: 0, textStyle: { color: '#a1a1aa', fontSize: 11 }, itemWidth: 12, itemHeight: 10 },
    color: ['#22c55e','#6366f1','#f59e0b','#71717a','#ef4444'],
    series: [{
      type: 'pie', radius: ['45%','72%'], center: ['50%','45%'],
      data: items,
      label: { show: true, color: '#a1a1aa', fontSize: 11, formatter: '{b}\n{d}%' },
      emphasis: { label: { fontSize: 13, fontWeight: 600 } },
    }],
  });
})();

// ── 4. Top authors bar ────────────────────────────────────────────────
(function() {
  const chart = initChart('chart-authors');
  if (!chart) return;
  const authors = DATA.topAuthors.slice().reverse();
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'axis', axisPointer: { type: 'shadow' } },
    grid: { left: 150, right: 40, top: 10, bottom: 10 },
    xAxis: { type: 'value', splitLine: { lineStyle: { color: '#27272a' } }, axisLabel: { color: '#71717a' } },
    yAxis: { type: 'category', data: authors.map(a => a.name), axisLabel: { color: '#a1a1aa', fontSize: 11 }, axisLine: { lineStyle: { color: '#3f3f46' } } },
    series: [{
      type: 'bar', data: authors.map(a => a.count),
      barWidth: '60%', itemStyle: { color: '#818cf8' },
      label: { show: true, position: 'right', color: '#a1a1aa', fontSize: 11, fontFamily: 'JetBrains Mono' },
    }],
  });
})();

// ── 5. Category x Year heatmap ────────────────────────────────────────
(function() {
  const chart = initChart('chart-cat-year');
  if (!chart) return;
  const years = Object.keys(DATA.timeline).map(Number);
  const catNames = DATA.categories.map(c => c.short);
  const heatData = [];
  let maxVal = 0;
  DATA.categories.forEach((cat, ci) => {
    years.forEach((y, yi) => {
      const v = cat.byYear[y] || 0;
      heatData.push([yi, ci, v]);
      if (v > maxVal) maxVal = v;
    });
  });
  chart.setOption({
    tooltip: { ...commonTooltip, formatter: p => p.data[2] > 0 ? catNames[p.data[1]] + ' (' + years[p.data[0]] + '): ' + p.data[2] + ' papers' : '' },
    grid: { left: 220, right: 60, top: 10, bottom: 30 },
    xAxis: { type: 'category', data: years, axisLabel: { color: '#71717a' }, splitArea: { show: true, areaStyle: { color: ['rgba(0,0,0,0)', 'rgba(255,255,255,0.02)'] } } },
    yAxis: { type: 'category', data: catNames, axisLabel: { color: '#a1a1aa', fontSize: 11 } },
    visualMap: { min: 0, max: maxVal, calculable: false, orient: 'vertical', right: 0, top: 'center',
      inRange: { color: ['#18181b','#312e81','#4f46e5','#818cf8','#c7d2fe'] },
      textStyle: { color: '#71717a' } },
    series: [{ type: 'heatmap', data: heatData, label: { show: true, color: '#a1a1aa', fontSize: 10, formatter: p => p.data[2] || '' },
      emphasis: { itemStyle: { shadowBlur: 10, shadowColor: 'rgba(99,102,241,0.4)' } } }],
  });
})();

// ── 6. Co-occurrence matrix ───────────────────────────────────────────
(function() {
  const chart = initChart('chart-cooccurrence');
  if (!chart) return;
  const labels = DATA.categories.map((c,i) => (i+1) + '. ' + c.short.substring(0,18));
  const heatData = [];
  let maxVal = 0;
  DATA.cooccurrence.forEach((row, ri) => {
    row.forEach((val, ci) => {
      if (ri !== ci) {
        heatData.push([ci, ri, val]);
        if (val > maxVal) maxVal = val;
      }
    });
  });
  chart.setOption({
    tooltip: { ...commonTooltip, formatter: p => {
      if (p.data[0] === p.data[1]) return '';
      return labels[p.data[1]] + ' & ' + labels[p.data[0]] + ': ' + p.data[2] + ' papers';
    } },
    grid: { left: 180, right: 60, top: 10, bottom: 100 },
    xAxis: { type: 'category', data: labels, axisLabel: { color: '#71717a', fontSize: 10, rotate: 45 }, splitArea: { show: true } },
    yAxis: { type: 'category', data: labels, axisLabel: { color: '#a1a1aa', fontSize: 10 } },
    visualMap: { min: 0, max: maxVal, calculable: false, orient: 'vertical', right: 0, top: 'center',
      inRange: { color: ['#18181b','#4a1d6e','#7c3aed','#a78bfa','#ddd6fe'] },
      textStyle: { color: '#71717a' } },
    series: [{ type: 'heatmap', data: heatData, label: { show: true, color: '#a1a1aa', fontSize: 10,
      formatter: p => p.data[2] || '' },
      emphasis: { itemStyle: { shadowBlur: 10 } } }],
  });
})();

// ── 7. Model bar chart ────────────────────────────────────────────────
(function() {
  const chart = initChart('chart-model-bar');
  if (!chart) return;
  const models = DATA.models.slice(0, 15).reverse();
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'axis', axisPointer: { type: 'shadow' } },
    grid: { left: 140, right: 40, top: 10, bottom: 10 },
    xAxis: { type: 'value', splitLine: { lineStyle: { color: '#27272a' } }, axisLabel: { color: '#71717a' } },
    yAxis: { type: 'category', data: models.map(m => m.name), axisLabel: { color: '#a1a1aa', fontSize: 11 }, axisLine: { lineStyle: { color: '#3f3f46' } } },
    series: [{
      type: 'bar', data: models.map(m => m.count),
      barWidth: '60%', itemStyle: { color: '#22d3ee' },
      label: { show: true, position: 'right', color: '#a1a1aa', fontSize: 11, fontFamily: 'JetBrains Mono' },
    }],
  });
})();

// ── 8. Method bar chart ───────────────────────────────────────────────
(function() {
  const chart = initChart('chart-method-bar');
  if (!chart) return;
  const methods = DATA.methods.slice().reverse();
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'axis', axisPointer: { type: 'shadow' } },
    grid: { left: 200, right: 40, top: 10, bottom: 10 },
    xAxis: { type: 'value', splitLine: { lineStyle: { color: '#27272a' } }, axisLabel: { color: '#71717a' } },
    yAxis: { type: 'category', data: methods.map(m => m.name), axisLabel: { color: '#a1a1aa', fontSize: 11 }, axisLine: { lineStyle: { color: '#3f3f46' } } },
    series: [{
      type: 'bar', data: methods.map(m => m.count),
      barWidth: '60%', itemStyle: { color: '#f59e0b' },
      label: { show: true, position: 'right', color: '#a1a1aa', fontSize: 11, fontFamily: 'JetBrains Mono' },
    }],
  });
})();

// ── 9. Model x Category heatmap ───────────────────────────────────────
(function() {
  const chart = initChart('chart-model-cat');
  if (!chart) return;
  const topModels = DATA.models.slice(0, 12);
  const modelNames = topModels.map(m => m.name);
  const catNames = DATA.categoryNames;
  const heatData = [];
  let maxVal = 0;
  topModels.forEach((m, mi) => {
    m.byCategory.forEach((v, ci) => {
      heatData.push([ci, mi, v]);
      if (v > maxVal) maxVal = v;
    });
  });
  chart.setOption({
    tooltip: { ...commonTooltip, formatter: p => modelNames[p.data[1]] + ' in ' + catNames[p.data[0]] + ': ' + p.data[2] + ' papers' },
    grid: { left: 140, right: 60, top: 10, bottom: 130 },
    xAxis: { type: 'category', data: catNames, axisLabel: { color: '#71717a', fontSize: 10, rotate: 40 } },
    yAxis: { type: 'category', data: modelNames, axisLabel: { color: '#a1a1aa', fontSize: 11 } },
    visualMap: { min: 0, max: maxVal, calculable: false, orient: 'vertical', right: 0, top: 'center',
      inRange: { color: ['#18181b','#164e63','#0891b2','#22d3ee','#cffafe'] },
      textStyle: { color: '#71717a' } },
    series: [{ type: 'heatmap', data: heatData, label: { show: true, color: '#a1a1aa', fontSize: 10,
      formatter: p => p.data[2] || '' },
      emphasis: { itemStyle: { shadowBlur: 10, shadowColor: 'rgba(34,211,238,0.3)' } } }],
  });
})();

// ── 10. Pluralistic donut ─────────────────────────────────────────────
(function() {
  const chart = initChart('chart-pluralistic');
  if (!chart) return;
  const d = DATA.pluralistic;
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'item', formatter: '{b}: {c} ({d}%)' },
    legend: { bottom: 0, textStyle: { color: '#a1a1aa', fontSize: 11 } },
    color: ['#22c55e','#f59e0b','#71717a'],
    series: [{
      type: 'pie', radius: ['45%','72%'], center: ['50%','42%'],
      data: [
        { name: 'Yes', value: d.Yes },
        { name: 'Partial', value: d.Partial },
        { name: 'No', value: d.No },
      ],
      label: { show: true, color: '#a1a1aa', formatter: '{b}\n{c} ({d}%)' },
      emphasis: { label: { fontSize: 13, fontWeight: 600 } },
    }],
  });
})();

// ── 11. Ethical traditions bar ─────────────────────────────────────────
(function() {
  const chart = initChart('chart-traditions');
  if (!chart) return;
  const trads = DATA.traditions.slice().reverse();
  const colorMap = { Critical: '#ef4444', Severe: '#f97316', Moderate: '#f59e0b', Adequate: '#22c55e' };
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'axis', axisPointer: { type: 'shadow' } },
    grid: { left: 160, right: 40, top: 10, bottom: 10 },
    xAxis: { type: 'value', splitLine: { lineStyle: { color: '#27272a' } }, axisLabel: { color: '#71717a' } },
    yAxis: { type: 'category', data: trads.map(t => t.name), axisLabel: { color: '#a1a1aa', fontSize: 11 }, axisLine: { lineStyle: { color: '#3f3f46' } } },
    series: [{
      type: 'bar', data: trads.map(t => ({ value: t.count, itemStyle: { color: colorMap[t.level] || '#71717a' } })),
      barWidth: '60%',
      label: { show: true, position: 'right', color: '#a1a1aa', fontSize: 11, fontFamily: 'JetBrains Mono',
        formatter: p => {
          const t = trads[p.dataIndex];
          return t.count + ' (' + t.level + ')';
        }
      },
    }],
  });
})();

// ── 12. Languages bar ─────────────────────────────────────────────────
(function() {
  const chart = initChart('chart-languages');
  if (!chart) return;
  const langs = DATA.languages.slice().reverse();
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'axis', axisPointer: { type: 'shadow' } },
    grid: { left: 120, right: 40, top: 10, bottom: 10 },
    xAxis: { type: 'value', splitLine: { lineStyle: { color: '#27272a' } }, axisLabel: { color: '#71717a' } },
    yAxis: { type: 'category', data: langs.map(l => l.name), axisLabel: { color: '#a1a1aa', fontSize: 11 }, axisLine: { lineStyle: { color: '#3f3f46' } } },
    series: [{
      type: 'bar', data: langs.map(l => l.count),
      barWidth: '60%', itemStyle: { color: '#14b8a6' },
      label: { show: true, position: 'right', color: '#a1a1aa', fontSize: 11, fontFamily: 'JetBrains Mono' },
    }],
  });
})();

// ── 13. Language x Category heatmap ───────────────────────────────────
(function() {
  const chart = initChart('chart-lang-cat');
  if (!chart) return;
  const topLangs = DATA.languages.slice(0, 10);
  const langNames = topLangs.map(l => l.name);
  const catNames = DATA.categoryNames;
  const heatData = [];
  let maxVal = 0;
  topLangs.forEach((l, li) => {
    l.byCategory.forEach((v, ci) => {
      heatData.push([ci, li, v]);
      if (v > maxVal) maxVal = v;
    });
  });
  chart.setOption({
    tooltip: { ...commonTooltip, formatter: p => langNames[p.data[1]] + ' in ' + catNames[p.data[0]] + ': ' + p.data[2] + ' papers' },
    grid: { left: 120, right: 60, top: 10, bottom: 130 },
    xAxis: { type: 'category', data: catNames, axisLabel: { color: '#71717a', fontSize: 10, rotate: 40 } },
    yAxis: { type: 'category', data: langNames, axisLabel: { color: '#a1a1aa', fontSize: 11 } },
    visualMap: { min: 0, max: Math.max(maxVal, 1), calculable: false, orient: 'vertical', right: 0, top: 'center',
      inRange: { color: ['#18181b','#134e4a','#0d9488','#14b8a6','#99f6e4'] },
      textStyle: { color: '#71717a' } },
    series: [{ type: 'heatmap', data: heatData, label: { show: true, color: '#a1a1aa', fontSize: 10,
      formatter: p => p.data[2] || '' } }],
  });
})();

// ── 14. Resources table ───────────────────────────────────────────────
(function() {
  const PAGE_SIZE = 25;
  let filtered = [...DATA.resources];
  let currentPage = 1;
  let sortCol = 'year';
  let sortDir = -1;

  // Populate dropdowns
  const catSet = new Set(DATA.resources.map(r => r.category));
  const catSel = document.getElementById('res-cat');
  [...catSet].sort().forEach(c => {
    const opt = document.createElement('option');
    opt.value = c; opt.textContent = c;
    catSel.appendChild(opt);
  });
  const yearSet = new Set(DATA.resources.map(r => r.year));
  const yearSel = document.getElementById('res-year');
  [...yearSet].sort((a,b) => b-a).forEach(y => {
    const opt = document.createElement('option');
    opt.value = y; opt.textContent = y;
    yearSel.appendChild(opt);
  });

  function applyFilters() {
    const search = document.getElementById('res-search').value.toLowerCase();
    const cat = catSel.value;
    const year = yearSel.value;
    const publicOnly = document.getElementById('res-public').checked;

    filtered = DATA.resources.filter(r => {
      if (search && !r.benchmark.toLowerCase().includes(search) && !r.what.toLowerCase().includes(search) && !r.filename.toLowerCase().includes(search)) return false;
      if (cat && r.category !== cat) return false;
      if (year && r.year !== parseInt(year)) return false;
      if (publicOnly && r.availability !== 'Public') return false;
      return true;
    });

    // Sort
    filtered.sort((a, b) => {
      let va = a[sortCol], vb = b[sortCol];
      if (typeof va === 'string') va = va.toLowerCase();
      if (typeof vb === 'string') vb = vb.toLowerCase();
      if (va < vb) return -sortDir;
      if (va > vb) return sortDir;
      return 0;
    });

    currentPage = 1;
    render();
  }

  function render() {
    const tbody = document.getElementById('res-tbody');
    const start = (currentPage - 1) * PAGE_SIZE;
    const page = filtered.slice(start, start + PAGE_SIZE);

    document.getElementById('res-count').textContent = filtered.length + ' results';

    tbody.innerHTML = page.map(r => {
      const badge = r.availability === 'Public' ? 'badge-public' : r.availability === 'Code available' ? 'badge-code' : 'badge-partial';
      return '<tr>'
        + '<td style="font-family:var(--font-mono)">' + r.year + '</td>'
        + '<td title="' + r.benchmark.replace(/"/g,'&quot;') + '">' + r.benchmark + '</td>'
        + '<td title="' + r.what.replace(/"/g,'&quot;') + '">' + r.what + '</td>'
        + '<td>' + r.category + '</td>'
        + '<td><span class="badge ' + badge + '">' + r.availability + '</span></td>'
        + '</tr>';
    }).join('');

    // Pagination
    const totalPages = Math.ceil(filtered.length / PAGE_SIZE);
    const pag = document.getElementById('res-pagination');
    if (totalPages <= 1) { pag.innerHTML = ''; return; }
    let html = '<button class="page-btn" ' + (currentPage<=1?'disabled':'') + ' onclick="window.__resPage(' + (currentPage-1) + ')">&laquo; Prev</button>';
    const start_p = Math.max(1, currentPage - 3);
    const end_p = Math.min(totalPages, currentPage + 3);
    for (let i = start_p; i <= end_p; i++) {
      html += '<button class="page-btn' + (i===currentPage?' active':'') + '" onclick="window.__resPage(' + i + ')">' + i + '</button>';
    }
    html += '<button class="page-btn" ' + (currentPage>=totalPages?'disabled':'') + ' onclick="window.__resPage(' + (currentPage+1) + ')">Next &raquo;</button>';
    pag.innerHTML = html;
  }

  window.__resPage = function(p) { currentPage = p; render(); };

  // Event listeners
  document.getElementById('res-search').addEventListener('input', applyFilters);
  catSel.addEventListener('change', applyFilters);
  yearSel.addEventListener('change', applyFilters);
  document.getElementById('res-public').addEventListener('change', applyFilters);

  // Column sort
  document.querySelectorAll('#res-table th[data-sort]').forEach(th => {
    th.addEventListener('click', () => {
      const col = th.dataset.sort;
      if (sortCol === col) sortDir *= -1;
      else { sortCol = col; sortDir = col === 'year' ? -1 : 1; }
      applyFilters();
    });
  });

  applyFilters();
})();

// ── 15. Normative donut ───────────────────────────────────────────────
(function() {
  const chart = initChart('chart-norm-donut');
  if (!chart) return;
  const d = DATA.normative;
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'item', formatter: '{b}: {c} ({d}%)' },
    legend: { bottom: 0, textStyle: { color: '#a1a1aa', fontSize: 11 } },
    color: ['#22c55e','#f59e0b','#3f3f46'],
    series: [{
      type: 'pie', radius: ['45%','72%'], center: ['50%','42%'],
      data: [
        { name: 'Yes (full)', value: d.yes },
        { name: 'Borderline', value: d.borderline },
        { name: 'Excluded', value: d.excluded },
      ],
      label: { show: true, color: '#a1a1aa', formatter: '{b}\n{c}' },
      emphasis: { label: { fontSize: 13, fontWeight: 600 } },
    }],
  });
})();

// ── 16. Framework bar chart ───────────────────────────────────────────
(function() {
  const chart = initChart('chart-norm-fw');
  if (!chart) return;
  const fws = DATA.normative.frameworks.slice().reverse();
  if (fws.length === 0) return;
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'axis', axisPointer: { type: 'shadow' } },
    grid: { left: 240, right: 40, top: 10, bottom: 10 },
    xAxis: { type: 'value', splitLine: { lineStyle: { color: '#27272a' } }, axisLabel: { color: '#71717a' } },
    yAxis: { type: 'category', data: fws.map(f => f.name), axisLabel: { color: '#a1a1aa', fontSize: 11 }, axisLine: { lineStyle: { color: '#3f3f46' } } },
    series: [{
      type: 'bar', data: fws.map(f => f.count),
      barWidth: '60%', itemStyle: { color: '#a855f7' },
      label: { show: true, position: 'right', color: '#a1a1aa', fontSize: 11, fontFamily: 'JetBrains Mono' },
    }],
  });
})();

// ── 17. Normative paper cards ─────────────────────────────────────────
(function() {
  const container = document.getElementById('norm-papers');
  if (DATA.normative.papers.length === 0) {
    container.innerHTML = '<p style="color:var(--text-muted)">No papers with full normative operationalization found.</p>';
    return;
  }
  container.innerHTML = DATA.normative.papers.map(p => {
    return '<div class="norm-card">'
      + '<h4>' + p.filename.replace('.md','') + '</h4>'
      + '<div class="meta">'
      + '<div><strong>Frameworks:</strong> <span>' + p.frameworks + '</span></div>'
      + '<div><strong>LLMs Tested:</strong> <span>' + p.llms + '</span></div>'
      + '<div><strong>Data Artifact:</strong> <span>' + p.artifact + '</span></div>'
      + '</div></div>';
  }).join('');
})();

// ── 18. LLM Assessment KPIs ──────────────────────────────────────────
(function() {
  const el1 = document.getElementById('llm-kpi-models');
  const el2 = document.getElementById('llm-kpi-urls');
  const el3 = document.getElementById('llm-kpi-pct');
  if (el1) el1.textContent = (DATA.kpi.llmModels || 0).toLocaleString();
  if (el2) el2.textContent = (DATA.kpi.dataUrls || 0).toLocaleString();
  if (el3) el3.textContent = (DATA.kpi.llmModelsPct || 0) + '%';
})();

// ── 19. LLM Model Frequency bar chart ────────────────────────────────
(function() {
  const chart = initChart('chart-llm-models');
  if (!chart || !DATA.llmModels || DATA.llmModels.length === 0) return;
  const models = DATA.llmModels.slice().reverse();
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'axis', axisPointer: { type: 'shadow' } },
    grid: { left: 140, right: 50, top: 10, bottom: 10 },
    xAxis: { type: 'value', splitLine: { lineStyle: { color: '#27272a' } }, axisLabel: { color: '#71717a' } },
    yAxis: { type: 'category', data: models.map(m => m.name), axisLabel: { color: '#a1a1aa', fontSize: 11 }, axisLine: { lineStyle: { color: '#3f3f46' } } },
    series: [{
      type: 'bar', data: models.map(m => m.count),
      barWidth: '60%', itemStyle: { color: '#22d3ee' },
      label: { show: true, position: 'right', color: '#a1a1aa', fontSize: 11, fontFamily: 'JetBrains Mono' },
    }],
  });
})();

// ── 20. Assessment Coverage by Year (stacked bar) ────────────────────
(function() {
  const chart = initChart('chart-llm-year');
  if (!chart || !DATA.llmByYear) return;
  const years = Object.keys(DATA.llmByYear).map(Number).sort();
  const assessed = years.map(y => DATA.llmByYear[y].assessed);
  const notAssessed = years.map(y => DATA.llmByYear[y].notAssessed);
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'axis' },
    legend: { top: 0, textStyle: { color: '#a1a1aa', fontSize: 11 }, itemWidth: 14, itemHeight: 10 },
    grid: { left: 50, right: 20, top: 40, bottom: 30 },
    xAxis: { type: 'category', data: years, axisLine: { lineStyle: { color: '#3f3f46' } }, axisLabel: { color: '#71717a' } },
    yAxis: { type: 'value', splitLine: { lineStyle: { color: '#27272a' } }, axisLabel: { color: '#71717a' } },
    series: [
      { name: 'No LLM Models', type: 'bar', stack: 'total', data: notAssessed, itemStyle: { color: '#3f3f46' }, barWidth: '60%' },
      { name: 'With LLM Models', type: 'bar', stack: 'total', data: assessed, itemStyle: { color: '#22d3ee' }, barWidth: '60%' },
    ],
  });
})();

// ── 21. Data Repository Distribution (donut) ─────────────────────────
(function() {
  const chart = initChart('chart-llm-repos');
  if (!chart || !DATA.repoDistribution || DATA.repoDistribution.length === 0) return;
  const REPO_COLORS = {
    'GitHub': '#22c55e', 'HuggingFace': '#f59e0b', 'Zenodo': '#6366f1',
    'OSF': '#ec4899', 'Kaggle': '#22d3ee', 'Other': '#71717a'
  };
  chart.setOption({
    tooltip: { ...commonTooltip, trigger: 'item', formatter: '{b}: {c} ({d}%)' },
    legend: { bottom: 0, textStyle: { color: '#a1a1aa', fontSize: 11 }, itemWidth: 12, itemHeight: 10 },
    series: [{
      type: 'pie', radius: ['45%','72%'], center: ['50%','45%'],
      data: DATA.repoDistribution.map(r => ({
        name: r.name, value: r.count,
        itemStyle: { color: REPO_COLORS[r.name] || '#71717a' }
      })),
      label: { show: true, color: '#a1a1aa', fontSize: 11, formatter: '{b}\n{c} ({d}%)' },
      emphasis: { label: { fontSize: 13, fontWeight: 600 } },
    }],
  });
})();
</script>
</body>
</html>